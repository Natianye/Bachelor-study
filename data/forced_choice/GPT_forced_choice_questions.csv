QA_id,content,questionA,questionB
0,"# Contributing to BayBE
## Writing classes
### Method overrides

When overriding methods in subclasses, decorate them with `@typing_extensions.override`
to make the relationship explicit:

```python
from typing_extensions import override

class Parent:

   def le_method():
      """"""The method of the parent class.""""""
      ...

class Child:

   @override
   def le_method():
      """"""Overridden method of the child class.""""""
      ...
```

Using the decorator provides a type-safe approach for defining inheritance structures
that eliminates two potential sources of unintended class design:

* An intended override is does not occur because the method names differ between
  the parent and child classes (e.g. if the parent method is renamed)
* An unintended override occurs because a method name that exists in the parent class
  is used in the child class by mistake.
  In both cases, `mypy` will complain and force you to fix the problem.",What decorator should you use when overriding methods in subclasses in BayBE to make the relationship explicit?,Mypy does not accept my definition of a function for a child class. What went wrong?
1,"# Contributing to BayBE

**All contributions to BayBE are welcome!**

... no matter if bug fixes, new features, or just typo corrections.

To shorten the overall development and review process, this page contains are a
few sections that can make your life easier.

## General Workflow

To implement your contributions in a local development environment,
we recommend the following workflow:

1. Clone a [fork](https://github.com/emdgroup/BayBE/fork) of the repository to
   your local machine.
2. Create and activate a virtual python environment using one of the supported
   python versions.
3. Change into the root folder of the cloned repository and install an editable version
   including all development dependencies:
   ```console
   pip install -e '.[dev]'
   ```
4. Run our tests to verify everything works as expected:
   ```console
   pytest
   ```
5. Install our [pre-commit](https://pre-commit.com/) hooks:
   ```console
   pre-commit install
   ```
6. Create a new branch for your contribution:
   ```console
   git checkout -b <your_branch_name>
   ```
7. **Implement your changes.**
8. Optional but recommended to prevent complaints from our CI pipeline:
   **Test your code.**

   There are several test environments you can run via `tox`, each corresponding to a
   [developer tool]() in a certain Python version.
   You can retrieve all available environments via `tox list`.
   For more information, see our [README about tests](https://github.com/emdgroup/baybe/blob/main/tests/README.md).

   For instance, running all code tests in Python 3.12 can be achieved via:
   ```console
   tox -e fulltest-py312
   ```

   Other tox tests that are useful to verify your work locally are `tox -e lint-py312`,
   `tox -e mypy-py312` and `tox -e coretest-py312`.

   If you want to challenge your machine, you can run all checks in all Python versions
   in parallel via:
   ```console
   tox -p
   ```

   This can be considered the ultimate one-stop check to make sure your code is ready
   for merge.
9. Push the updated branch back to your fork:
   ```console
   git push origin
   ```
10. Open a pull request via Githubâs web page.",Can I make changes to improve the BayBE repo?,How can I run all code tests for BayBE in Python 3.12?
2,"# Contributing to BayBE
## Synchronizing Pull Requests

A common situation encountered when submitting a pull request (PR) is that the upstream
branch has evolved since the moment your PR branch was created, and a synchronization
is needed in order to prepare your branch for a merge (e.g., to remove existing
conflicts).

Because we care about our Git history and would like to keep it clean and
easy to follow, we generally recommend **rebasing** your branch onto the latest
upstream commit in such situations, especially if your changes are orthogonal to what
has happened on the remote branch in the meantime. Compared to merging, this has the
advantage of keeping the history of your commits (and thus of the entire repository)
linear, and your own PR free of changes that happened remotely, which also
greatly simplifies the review process (e.g., it produces simpler diffs).

That said, the above is only a recommendation and by no means a requirement. However,
depending on the complexity of your PR commit history, we reserve the right to merge
your branch using a squash-rebase as a last resort to keep our history clean.
By following the guideline above, this step can be easily avoided in most cases.

<a id=""developer-tools""></a>",How should I prepare my pull request?,What is the recommended method for synchronizing a BayBE pull request branch with the latest upstream changes to keep the Git history clean?
3,"# Contributing to BayBE
## Developer Tools

In order to maintain a high code quality, we use a variety of code developer tools.
When following the above described workflow, [pre-commit](https://pre-commit.com/)
will automatically trigger (most) necessary checks during your development process.
In any case, these checks are also conducted in our CI pipeline, which must pass
before your pull request is considered ready for review.
If you have questions or problems, simply ask for advice.

| Tool                                                                                            | Purpose                                   |
|-------------------------------------------------------------------------------------------------|-------------------------------------------|
| [ruff](https://docs.astral.sh/ruff/)                                                            | code linting and formatting               |
| [mypy](https://mypy.readthedocs.io/)                                                            | static type checking                      |
| [pydocstyle](http://www.pydocstyle.org/)   <br/> [pydoclint](https://github.com/jsh9/pydoclint) | analyzing docstrings                      |
| [typos](https://github.com/crate-ci/typos)                                                      | basic spell checking                      |
| [pytest](https://docs.pytest.org/)                                                              | testing                                   |
| [pytest-cov](https://pytest-cov.readthedocs.io/)                                                | measuring test coverage                   |
| [sphinx](https://www.sphinx-doc.org/)                                                           | generating our documentation              |
| [pip-audit](https://github.com/pypa/pip-audit)                                                  | detecting vulnerabilities in dependencies |
| [tox](https://tox.wiki/)                                                                        | orchestrating all the above               |

Executing a specific one of these tools is easiest by using the corresponding
[tox](https://tox.wiki/) environment,

```console
tox -e <env>
```

where `<env>` is any of the environment names found via `tox list`.

<a id=""code-design""></a>",What tool does BayBE use to detect vulnerabilities in dependencies?,Do I have to install sphinx separately?
4,"# Contributing to BayBE
## Code Design

When reading BayBEâs code, you will notice certain re-occurring design patterns.
These patterns are by no means enforced, but following them can streamline your
own development process:

* We build most our classes with [attrs](https://www.attrs.org/), which is useful
  for lean class design and attribute validation.
* Our (de-)serialization machinery is built upon [cattrs](https://catt.rs/), separating
  object serialization from class design.
* The modular nature of BayBEâs components is reflected in our test suite through
  the use of [hypothesis](https://hypothesis.readthedocs.io/) property tests.",What library does BayBE use for property-based testing in its test suite?,What are the attrs and cattrs libraries used for?
5,"# Contributing to BayBE
## Extending BayBEâs Functionality

For most parts, BayBEâs code and functional components are organized into different
subpackages.
When extending its functionality (for instance, by adding new component subclasses),
make sure that the newly written code is well integrated into the existing package and
module hierarchy.
In particular, public functionality should be imported into the appropriate high-level
namespaces for easier user import. For an example, see our
[parameter namespace](https://github.com/emdgroup/baybe/blob/main/baybe/parameters/__init__.py).",Where should new public functionality be imported when extending BayBEâs functionality?,What are supported parameter types in BayBE and what are their names?
6,"# Contributing to BayBE
## Writing Docstrings

Our docstrings generally follow the
[Google Python Style Guide](https://google.github.io/styleguide/pyguide.html).
Basic style and consistency checks are automatically performed via
[pre-commit](https://pre-commit.com/) during development and in our CI pipeline.

Apart from that, we generally recommend adhering to the following guideline:

- Each function should have a docstring containing:
  * a short one-line summary at the top,
  * an optional extended summary or description below and
  * all relevant sections (`Args`, `Raises`, ...).
- Use type hints (for variables/constants, attributes, function/method signatures, ...).
  Avoid repeating type hints in docstrings.
- When referencing objects (classes, functions, ...),
  use `:<key>:`path.to.function` ` where `<key>` is to be replaced with the
  respective [role](https://www.sphinx-doc.org/en/master/usage/domains/python.html#cross-referencing-python-objects)
  (`class`, `func`, ...)
- Use double backticks for literals like in ```MyString```.",What is the rule to write docstrings?,What style guide do BayBE docstrings generally follow?
7,"# Contributing to BayBE
## Writing classes
### Conventions for `attrs` classes

- Place attribute docstrings below the attribute declaration, not in the class
  docstring.
  Separate different attributes using a blank line.
  For example:
  ```python
  @define
  class Cookies:
    """"""A delicious recipe for chocolate-banana cookies.""""""

    chocolate: float
    """"""Chocolate is naturally measured in terms of floats.""""""

    bananas: int
    """"""For bananas, we use integers, of course.""""""
  ```
- Unless another more specific name is suitable, use our default naming convention for
  `attrs` defaults and validators:
  ```python
  @my_attribute.default
  def _default_my_attribute(self): ...

  @my_attribute.validator
  def _validate_my_attribute(self, attribute, value): ...
  ```

  A one-line docstring suffices for these methods, but they should have a `Raises:`
  section if applicable. Linter warnings regarding missing attribute docstrings can be
  silenced using `# noqa: DOC101, DOC103`.",Where should attribute docstrings be placed when writing `attrs` classes in BayBE?,What is the docstring convention in BayBE?
8,"# FAQ
### Do I need to create a campaign to get recommendations?

No, creating a campaign is not mandatory.
BayBE offers two entry points for generating recommendations:

* a stateful [`Campaign.recommend`]() method and
* a stateless [`RecommenderProtocol.recommend`]() method.

",Is it necessary to create a campaign to get recommendations in BayBE?,Do I need to create a campaign to get recommendations?
9,"# FAQ
### BayBE recommends A but experimentalists do B. What now?

Donât panic and grab your towel. Recommendations from BayBE are just ... well,
ârecommendationsâ. The measurements you feed back to BayBE need not to be related to
the original recommendation in any way. In fact, requesting recommendations and adding
data are two separate actions, and there is no formal requirement to perform these
actions in any particular order nor to ârespondâ to recommendations in any form.

Note, however, that subsequent recommendations **may** be affected by earlier steps in
your campaign, depending on your settings for the
`allow_recommending_already_measured` and
`allow_recommending_already_recommended` flags.","BayBE recommends A, but experimentalists do B. What now?",Do I have to follow BayBE's recommendations exactly when running my experiments?
10,"# BayBE â A Bayesian Back End for Design of Experiments
## ð» Installation
### From Package Index

The easiest way to install BayBE is via PyPI:

```bash
pip install baybe
```

A certain released version of the package can be installed by specifying the
corresponding version tag in the form `baybe==x.y.z`.",How can I install BayBE using pip?,How to install BayBE?
11,"# BayBE â A Bayesian Back End for Design of Experiments
## ð» Installation
### From GitHub

If you need finer control and would like to install a specific commit that has not been
released under a certain version tag, you can do so by installing BayBE directly from
GitHub via git and specifying the corresponding
[git ref](https://pip.pypa.io/en/stable/topics/vcs-support/#git).

For instance, to install the latest commit of the main branch, run:

```bash
pip install git+https://github.com/emdgroup/baybe.git@main
```",How can I install the latest commit of BayBE from GitHub?,How to install BayBE from Github?
12,"# BayBE â A Bayesian Back End for Design of Experiments
## ð» Installation
### From Local Clone

Alternatively, you can install the package from your own local copy.
First, clone the repository, navigate to the repository root folder, check out the
desired commit, and run:

```bash
pip install .
```

A developer would typically also install the package in editable mode (â-eâ),
which ensures that changes to the code do not require a reinstallation.

```bash
pip install -e .
```

If you need to add additional dependencies, make sure to use the correct syntax
including `''`:

```bash
pip install -e '.[dev]'
```","I want to contribute to the BayBE project by fixing a bug or adding a new feature. How should I install the package so that I can test my code changes live without having to reinstall it every time I edit a file? Also, what is the correct command to ensure I also get all the extra dependencies required for development and running tests?",How do you install BayBE in editable mode from a local clone?
13,"# BayBE â A Bayesian Back End for Design of Experiments
## ð» Installation
### Optional Dependencies

There are several dependency groups that can be selected during pip installation, like

```bash
pip install 'baybe[test,lint]' # will install baybe with additional dependency groups `test` and `lint`
```

To get the most out of `baybe`, we recommend to install at least

```bash
pip install 'baybe[chem,simulation]'
```

The available groups are:

- `extras`: Installs all dependencies required for optional features.
- `benchmarking`: Required for running the benchmarking module.
- `chem`: Cheminformatics utilities (e.g. for the `SubstanceParameter`).
- `docs`: Required for creating the documentation.
- `examples`: Required for running the examples/streamlit.
- `lint`: Required for linting and formatting.
- `mypy`: Required for static type checking.
- `onnx`: Required for using custom surrogate models in [ONNX format](https://onnx.ai).
- `polars`: Required for optimized search space construction via [Polars](https://docs.pola.rs/).
- `insights`: Required for built-in model and campaign analysis (e.g. using [SHAP](https://shap.readthedocs.io/)).
- `simulation`: Enabling the [simulation](https://emdgroup.github.io/baybe/stable/_autosummary/baybe.simulation.html) module.
- `test`: Required for running the tests.
- `dev`: All of the above plus dev tools. For code contributors.",How do I install BayBE? Walk me through the installation process for BayBE.,Which optional dependency group should I install to use cheminformatics utilities in BayBE?
14,"# BayBE â A Bayesian Back End for Design of Experiments
## ð¡ Telemetry

BayBE collects anonymous usage statistics **only** for employees of Merck KGaA,
Darmstadt, Germany and/or its affiliates. The recording of metrics is turned off for
all other users and is impossible due to a VPN block. In any case, the usage statistics
do **not** involve logging of recorded measurements, targets/parameters or their names
or any project information that would allow for reconstruction of details. The user and
host machine names are anonymized with via truncated hashing.

- You can verify the above statements by studying the open-source code in the
  `telemetry` module.
- You can always deactivate all telemetry by setting the environment variable
  `BAYBE_TELEMETRY_ENABLED` to `false` or `off`. For details please consult
  [this page](https://emdgroup.github.io/baybe/stable/userguide/envvars.html#telemetry).
- If you want to be absolutely sure, you can uninstall internet related packages such
  as `opentelemetry*` or its secondary dependencies from the environment. Due to the
  inability of specifying opt-out dependencies, these are installed by default, but the
  package works without them.",Does BayBE collect usage statistics?,How can I disable all telemetry in BayBE?
15,"# BayBE â A Bayesian Back End for Design of Experiments

The **Bay**esian **B**ack **E**nd (**BayBE**) is a general-purpose toolbox for Bayesian Design
of Experiments, focusing on additions that enable real-world experimental campaigns.

## ð Batteries Included

Besides its core functionality to perform a typical recommend-measure loop, BayBE
offers a range of â¨**builtâinÂ features**â¨ crucial for real-world use cases.
The following provides a non-comprehensive overview:

- ð ï¸ Custom parameter encodings: Improve your campaign with domain knowledge
- ð§ª Built-in chemical encodings: Improve your campaign with chemical knowledge
- ð¯ Numerical and binary targets with min, max and match objectives
- âï¸  Multi-target support via Pareto optimization and desirability scalarization
- ð Insights: Easily analyze feature importance and model behavior
- ð­ Hybrid (mixed continuous and discrete) spaces
- ð Transfer learning: Mix data from multiple campaigns and accelerate optimization
- ð° Bandit models: Efficiently find the best among many options in noisy environments (e.g. A/B Testing)
- ð¢ Cardinality constraints: Control the number of active factors in your design
- ð Distributed workflows: Run campaigns asynchronously with pending experiments and partial measurements
- ð Active learning: Perform smart data acquisition campaigns
- âï¸ Custom surrogate models: Enhance your predictions through mechanistic understanding
- ð Comprehensive backtest, simulation and imputation utilities: Benchmark and find your best settings
- ð Fully typed and hypothesis-tested: Robust code base
- ð All objects are fully (de-)serializable: Useful for storing results in databases or use in wrappers like APIs",What type of optimization methods does BayBE offer for handling multiple targets in experiments?,Which kinds of targets are supported by BayBE?
16,"# BayBE â A Bayesian Back End for Design of Experiments
## â¡ Quick Start
Let us consider a simple experiment where we control three parameters and want to
maximize a single target called `Yield`.

First, install BayBE into your Python environment:

```bash
pip install baybe
```
### Defining the Optimization Objective

In BayBEâs language, the `Yield` can be represented as a `NumericalTarget`,
which we wrap into a `SingleTargetObjective`:

```python
from baybe.targets import NumericalTarget
from baybe.objectives import SingleTargetObjective

target = NumericalTarget(
    name=""Yield"",
    mode=""MAX"",
)
objective = SingleTargetObjective(target=target)
```

In cases where we are confronted with multiple (potentially conflicting) targets,
the `ParetoObjective` or `DesirabilityObjective` can be used instead.
These allow to define additional settings, such as how the targets should be balanced.",Give me a code example on how to get started with BayBE,What Python command is used to install BayBE?
17,"# BayBE â A Bayesian Back End for Design of Experiments
## â¡ Quick Start
### Defining the Search Space

Next, we inform BayBE about the available âcontrol knobsâ, that is, the underlying
system parameters we can tune to optimize our targets. This also involves specifying
their values/ranges and other parameter-specific details.

For our example, we assume that we can control three parameters â `Granularity`,
`Pressure[bar]`, and `Solvent` â as follows:

```python
from baybe.parameters import (
    CategoricalParameter,
    NumericalDiscreteParameter,
    SubstanceParameter,
)

parameters = [
    CategoricalParameter(
        name=""Granularity"",
        values=[""coarse"", ""medium"", ""fine""],
        encoding=""OHE"",  # one-hot encoding of categories
    ),
    NumericalDiscreteParameter(
        name=""Pressure[bar]"",
        values=[1, 5, 10],
        tolerance=0.2,  # allows experimental inaccuracies up to 0.2 when reading values
    ),
    SubstanceParameter(
        name=""Solvent"",
        data={
            ""Solvent A"": ""COC"",
            ""Solvent B"": ""CCC"",  # label-SMILES pairs
            ""Solvent C"": ""O"",
            ""Solvent D"": ""CS(=O)C"",
        },
        encoding=""MORDRED"",  # chemical encoding via scikit-fingerprints
    ),
]
```

For more parameter types and their details, see the
[parameters section](https://emdgroup.github.io/baybe/stable/userguide/parameters.html)
of the user guide.

Additionally, we can define a set of constraints to further specify allowed ranges and
relationships between our parameters. Details can be found in the
[constraints section](https://emdgroup.github.io/baybe/stable/userguide/constraints.html) of the user guide.
In this example, we assume no further constraints.

With the parameter definitions at hand, we can now create our
`SearchSpace` based on the Cartesian product of all possible parameter values:

```python
from baybe.searchspace import SearchSpace

searchspace = SearchSpace.from_product(parameters)
```

See the [search spaces section](https://emdgroup.github.io/baybe/stable/userguide/searchspace.html)
of our user guide for more information on the structure of search spaces
and alternative ways of construction.",How do I inform BayBE about the parameters we can tune to optimize our targets?,"What encoding method is used for the ""Granularity"" parameter in BayBE's search space definition?"
18,"# BayBE â A Bayesian Back End for Design of Experiments
## â¡ Quick Start
### Optional: Defining the Optimization Strategy

As an optional step, we can specify details on how the optimization should be
conducted. If omitted, BayBE will choose a default setting.

For our example, we combine two recommenders via a so-called meta recommender named
`TwoPhaseMetaRecommender`:

1. In cases where no measurements have been made prior to the interaction with BayBE,
   a selection via `initial_recommender` is used.
2. As soon as the first measurements are available, we switch to `recommender`.

For more details on the different recommenders, their underlying algorithmic
details, and their configuration settings, see the
[recommenders section](https://emdgroup.github.io/baybe/stable/userguide/recommenders.html)
of the user guide.

```python
from baybe.recommenders import (
    BotorchRecommender,
    FPSRecommender,
    TwoPhaseMetaRecommender,
)

recommender = TwoPhaseMetaRecommender(
    initial_recommender=FPSRecommender(),  # farthest point sampling
    recommender=BotorchRecommender(),  # Bayesian model-based optimization
)
```",What is the purpose of using the TwoPhaseMetaRecommender in BayBE?,"Can I combine two recommenders? If yes, how do I do that?"
19,"# BayBE â A Bayesian Back End for Design of Experiments
## â¡ Quick Start
### The Optimization Loop

We can now construct a campaign object that brings all pieces of the puzzle together:

```python
from baybe import Campaign

campaign = Campaign(searchspace, objective, recommender)
```

With this object at hand, we can start our experimentation cycle.
In particular:

* We can ask BayBE to `recommend` new experiments.
* We can `add_measurements` for certain experimental settings to the campaignâs
  database.

Note that these two steps can be performed in any order.
In particular, available measurements can be submitted at any time and also several
times before querying the next recommendations.

```python
df = campaign.recommend(batch_size=3)
print(df)
```

```none
   Granularity  Pressure[bar]    Solvent
15      medium            1.0  Solvent D
10      coarse           10.0  Solvent C
29        fine            5.0  Solvent B
```

Note that the specific recommendations will depend on both the data
already fed to the campaign and the random number generator seed that is used.

After having conducted the corresponding experiments, we can add our measured
targets to the table and feed it back to the campaign:

```python
df[""Yield""] = [79.8, 54.1, 59.4]
campaign.add_measurements(df)
```

With the newly arrived data, BayBE can produce a refined design for the next iteration.
This loop would typically continue until a desired target value has been achieved in
the experiment.",How do you add new experimental measurements to a BayBE campaign?,What can we do with a campaign object?
20,"# BayBE â A Bayesian Back End for Design of Experiments
## â¡ Quick Start
### Advanced Example: Chemical Substances

BayBE has several modules to go beyond traditional approaches. One such example is the
use of custom encodings for categorical parameters. Chemical encodings for substances
are a special built-in case of this that comes with BayBE.

In the following picture you can see
the outcome for treating the solvent, base and ligand in a direct arylation reaction
optimization (from [Shields, B.J. et al.](https://doi.org/10.1038/s41586-021-03213-y)) with
chemical encodings compared to one-hot and a random baseline:
![Substance Encoding Example](examples/Backtesting/full_lookup_light.svg)
",Does BayBE have a special way to encode chemical substances?,What built-in feature does BayBE provide for encoding chemical substances in experiments?
21,"# Known Issues
## Installation Related Issues
### macOS-arm64 â Leaked Semaphore

We know of a number of instances where BayBE fails during runtime on macOS-arm64
systems. In particular M1 seems to be affected.

The issues often contain a reference to `semaphore`, e.g.
`UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown`.
While we do not know the exact source of the problem, it seems to be related to linked
libraries that need to be compiled from source when no `macOS-arm64` binaries are
available. Packages that seem to have regular problems are `pymatgen` or `matminer`.  :class: tip
Install `baybe` into a clean environment without pre-existing
packages. If you require other packages, try to install `baybe` first.

",I have issues installing BayBE on my Mac. How can I solve them?,"What issue might BayBE users encounter when running on macOS-arm64 systems, especially with M1 chips?"
22,"# Known Issues
## Installation Related Issues
### CPUs without AVX support â Installation of `polars`

The package `polars` that can be installed as an optional dependency is only supported for
CPUs with AVX support. As a consequence, you might not be able to install the optional dependency.
This is in particular the case for M1 Macs, as these do not offer this support.  :class: tip
Instead of `polars`, install `polars-lts-cpu`. BayBE will automatically detect the
presence of `polars` and active its advanced machinery. For more details, we refer to the
[polars installation guide](https://docs.pola.rs/user-guide/installation/).",What should I install instead of polars on CPUs without AVX support for BayBE?,"My installation failed, when I tried installing polars. What do I do?"
23,"# Known Issues
## Installation Related Issues

### Windows â Torch Problems

Reports of crashes during runtime on Windows machines often stem from a faulty `torch`
installation, e.g. wrongly installed CUDA-`torch` combinations. Errors look like
`OSError: [WinError 126] The specified module was not found. Error loading  C:\Users\xxxx\AppData\Roaming\Python\Python310\site-packages\torch\lib\shm.dll or one of its dependencies`",I have problems with CUDA. What shall I do?,What is a common cause of runtime crashes related to torch on Windows machines when using BayBE?
24,"# Known Issues
## PyCharm vs. `exceptiongroup`

BayBEâs (de-)serialization machinery is build upon `cattrs`, which in turn relies on
`ExceptionGroup`s to report problems in a nicely structured format when using its
[detailed validation](https://catt.rs/en/stable/validation.html#detailed-validation)
feature. However, `ExceptionGroup`s were introduced in Python 3.11 and are
therefore not usable with earlier Python versions. To
enable the feature nevertheless, `cattrs` uses the [exceptiongroup
backport](https://pypi.org/project/exceptiongroup/), which enables the same
functionality by monkeypatching `TracebackException` and installing a special
exception hook on `sys.excepthook`.

The changes attempted by `exceptiongroup` will only be executed if **no prior
modifications have been made**. However, PyCharm appears to make similar modifications
for its own purposes, blocking those of `exceptiongroup` and thus preventing the
exceptions from being properly thrown in detailed validation mode.

The chances of encountering this problem when interacting with BayBE are rather low
as the (de-)serialization objects are usually created by BayBE itself under normal
operation, so there is little risk of them being invalid in the first place. A
potential situation where you might run into the problem is if you manually
write a BayBE configuration and try to deserialize it into a Python BayBE object.
This can happen, for example, while engineering the configuration for later API
calls and testing it locally **using PyCharm**.  :class: tip
You can use **any** of the following workarounds to circumvent the problem:
* Run the code from the terminal instead of inside PyCharm
* Change PyCharm's run configuration from ""Run with Python Console"" to ""Emulate
  terminal in output console""
* Use Python version 3.11 or higher
* Undo the monkeypatch applied by PyCharm by running the following code **at the start
  of your script**:
    ```python
    import sys
    sys.excepthook = sys.__excepthook__
    ```
* Manually [format the exception](https://github.com/agronholm/exceptiongroup/blob/8b8791b662c0f62a574a09f305cd204dfb0a6a05/README.rst?plain=1) thrown by the problematic code:
    ```python
    import exceptiongroup
    from cattrs import ClassValidationError
    
    try:
    <problematic code>
    except ClassValidationError as e:
    raise ValueError("""".join(exceptiongroup.format_exception(e)))
    ```",What Python version introduced ExceptionGroup support used by BayBE's (de-)serialization machinery?,I don't understand the exception error in my PyCharm.
25,"# Contributors
## Maintainers

- Martin Fitzner (Merck KGaA, Darmstadt, Germany), [Contact](mailto:martin.fitzner@merckgroup.com), [Github](https://github.com/Scienfitz)
- Adrian Å oÅ¡iÄ (Merck Life Science KGaA, Darmstadt, Germany), [Contact](mailto:adrian.sosic@merckgroup.com), [Github](https://github.com/AdrianSosic)
- Alexander Hopp (Merck KGaA, Darmstadt, Germany), [Contact](mailto:alexander.hopp@merckgroup.com), [Github](https://github.com/AVHopp)",Who can I thank for this awesome package?,Who are the maintainers of BayBE?
26,"# Contributors
## Contributors

- Alex Lee (EMD Electronics, Tempe, Arizona, USA):<br />
  \\\\
  Work on surrogate models
- Daniel Weber (Merck KGaA, Darmstadt, Germany):<br />
  \\\\
  Telemetry prototype
- Emeline Sola (during an internship at Merck KGaA, Darmstadt, Germany):<br />
  \\\\
  Auto-documentation of the examples
- Sourabh Agrawal (Sigma-Aldrich Chemicals Private Limited):<br />
  \\\\
  Initial implementation of additional surrogate models and clustering methods
- Julie Fang (Merck Life Science KGaA, Darmstadt, Germany):<br />
  \\\\
  Farthest point sampling
- Roya Javadi (Vector Institute, Toronto, Canada):<br />
  \\\\
  Import optimization, Polars implementations
- Sterling Baird (Acceleration Consortium, Toronto, Canada):<br />
  \\\\
  Documentation and general feedback
- Rim Rihana (Merck KGaA, Darmstadt, Germany):<br />
  \\\\
  Human readable output for search spaces
- Di Jin (Merck Life Science KGaA, Darmstadt, Germany):<br />
  \\\\
  Cardinality constraints
- Julian Streibel (Merck Life Science KGaA, Darmstadt, Germany):<br />
  \\\\
  Bernoulli multi-armed bandit and Thompson sampling
- Karin Hrovatin (Merck KGaA, Darmstadt, Germany):<br />
  \\\\
  `scikit-fingerprints` support
- Fabian Liebig (Merck KGaA, Darmstadt, Germany):<br />
  \\\\
  Benchmarking structure and persistence capabilities for benchmarking results
- Alexander Wieczorek (Swiss Federal Institute for Materials Science and Technology, DÃ¼bendorf, Switzerland):<br />
  \\\\
  SHAP explainers for insights",Who contributed the Bernoulli multi-armed bandit and Thompson sampling to BayBE?,Who are the contributors of BayBE?
27,"# License
Copyright 2022-2025 Merck KGaA, Darmstadt, Germany and/or its affiliates. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0   Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
```text

                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      ""License"" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      ""Licensor"" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      ""Legal Entity"" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      ""control"" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      ""You"" (or ""Your"") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      ""Source"" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      ""Object"" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      ""Work"" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      ""Derivative Works"" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      ""Contribution"" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, ""submitted""
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as ""Not a Contribution.""

      ""Contributor"" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a ""NOTICE"" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an ""AS IS"" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS
```",Under which license is BayBE distributed?,What legal rules I must follow when I use or distribute BayBE?
28,"# Active Learning

When deciding which experiments to perform next, e.g. for a data acquisition campaign
to gather data for a machine learning model, it can be beneficial to follow a guided
approach rather than selecting experiments randomly. If this is done via iteratively
measuring points according to a criterion reflecting the current modelâs uncertainty,
the method is called **active learning**.

Active learning can be seen as a special case of Bayesian optimization: If we have the
above-mentioned criterion and set up a Bayesian optimization campaign to recommend
points with the highest uncertainty, we achieve active learning via Bayesian
optimization. In practice, this is procedure is implemented by setting up a
probabilistic model of our measurement process that allows us to quantify uncertainty
in the form of a posterior distribution, from which we can then construct an
uncertainty-based acquisition function to guide the exploration process.",What is active learning in the context of experiment selection for machine learning data acquisition?,How can I make sure that my model learns as much as possible about a process?
29,"# Active Learning
## Local Uncertainty Reduction

In BayBE, there are two types of acquisition function that can be chosen to search for
the points with the highest predicted model uncertainty:

- [`PosteriorStandardDeviation`]() (`PSTD`)
  / [`qPosteriorStandardDeviation`]() (`qPSTD`)
- [`UpperConfidenceBound`]() (`UCB`) /
  [`qUpperConfidenceBound`]() (`qUCB`)
  with high `beta`:<br />
  \\\\
  Increasing values of `beta` effectively eliminate the effect of the posterior mean on
  the acquisition value, yielding a selection of points driven primarily by the
  posterior variance. However, we generally recommend to use this acquisition function
  only if a small exploratory component is desired â otherwise, the
  [`qPosteriorStandardDeviation`]()
  acquisition function is what you are looking for.",What acquisition function does BayBE recommend for selecting points with the highest predicted model uncertainty?,How do I tweak parameters of the UCB acquisition function
30,"# Active Learning
## Global Uncertainty Reduction

BayBE also offers the
[`qNegIntegratedPosteriorVariance`]()
(`qNIPV`), which integrates
the posterior variance over the entire search space.
Choosing candidates based on this acquisition function is tantamount to selecting the
set of points resulting in the largest reduction of global uncertainty when added to
the already existing experimental design.

Because of its ability to quantify uncertainty on a global scale, this approach is often
superior to using a point-based uncertainty criterion as acquisition function.
However, due to its computational complexity, it can be prohibitive to integrate over
the entire search space. For this reason, we offer the option to sub-sample parts of it,
configurable via the constructor:

```python
from baybe.acquisition import qNIPV
from baybe.utils.sampling_algorithms import DiscreteSamplingMethod

# Will integrate over the entire search space
qNIPV()

# Will integrate over 50% of the search space, randomly sampled
qNIPV(sampling_fraction=0.5)

# Will integrate over 250 points, chosen by farthest point sampling
# Both lines are equivalent
qNIPV(sampling_n_points=250, sampling_method=""FPS"")
qNIPV(sampling_n_points=250, sampling_method=DiscreteSamplingMethod.FPS)
```:class: note
Sampling of the continuous part of the search space will always be random, while 
sampling of the discrete part can be controlled by providing a corresponding 
[`DiscreteSamplingMethod`](baybe.utils.sampling_algorithms.DiscreteSamplingMethod) for 
`sampling_method`.:class: important
Please be aware that in case of a purely continuous search space, the number of points 
to sample for integration must be specified via `sampling_n_points` (since providing
a fraction becomes meaningless).",What is a good fraction of random sampling to replace partly the integration of the search space?,What does the qNIPV acquisition function in BayBE integrate to select candidates for global uncertainty reduction?
31,"# Asynchronous Workflows

Asynchronous workflows describe situations where the loop between measurement and
recommendation is more complex and needs to incorporate various other aspects. These
could for instance be:

- **Distributed workflows**: When recommendations are distributed across several
  operators, e.g. at different locations or in several reactors, some experiments might
  have been started, but are not ready when the next batch of recommendations is requested.
  Without further consideration, the algorithm would be likely to recommend the pending
  experiments again (since they were and still are considered most promising), as it is
  unaware they were already started.
- **Partial targets**: When dealing with multiple targets that require very different
  amounts of time to measure, the targets of previously recommended points might only be
  partially available when requesting the next batch of recommendations. Still, these
  partial experiments should ideally be considered when generating the recommendations.

With *pending experiments* we mean experiments whose measurement process has
been started, but not yet completed by time of triggering the next set of
recommendations â this is typically the case when at least one of the configured
targets has not yet been measured.

There are two levels of dealing with such situations:

1. **Marking experiments as pending**: If an experiment is not completed (meaning at least one target is not yet measured), its
   data cannot be added as a regular measurement. However, it can be marked as pending via
   `pending_experiments` in `recommend`.
2. **Adding partial results**: If an experiment is partially completed (meaning at least one target has been
   measured), we can already update the model with the available information
   by adding a *partial* measurement.
","What does it mean for an experiment to be marked as ""pending"" in BayBE's asynchronous workflows?",What can I do if some of my experiments are still running but I need new recommendations for additional experiments?
32,"# Asynchronous Workflows
## Marking Experiments as Pending

To avoid repeated recommendations in the above scenario, BayBE provides the
`pending_experiments` keyword. It is available wherever recommendations can be
requested, i.e. [`Campaign.recommend`]() or
[`RecommenderProtocol.recommend`]().

Akin to `measurements` or `recommendations`, `pending_experiments` is a dataframe in
[experimental representation](searchspace.md#data-representation).
In the following example, we get a set of recommendations, add results for half of them,
and start the next recommendation, marking the other half pending:

```python
from baybe.utils.dataframe import add_fake_measurements

# Get a set of 10 recommendation
rec = campaign.recommend(batch_size=10)

# Split recommendations into two parts
rec_finished = rec.iloc[:5]
rec_pending = rec.iloc[5:]

# Add target measurements to the finished part. Here we add fake results
add_fake_measurements(rec_finished, campaign.targets)
campaign.add_measurements(rec_finished)

# Get the next set of recommendations, incorporating the still unfinished experiments.
# These will not include the experiments marked as pending again.
rec_next = campaign.recommend(10, pending_experiments=rec_pending)
```","What are ""pending_experiments"" good for?",What is the purpose of the pending_experiments keyword in BayBE?
33,"# Asynchronous Workflows
## Adding Partial Results

A *partial result* is possible if you have multiple targets, but only measured the
outcome for some of those. This is a common occurrence, especially if the different
target measurements correspond to experiments that differ in complexity or duration.

As a simple example, consider a campaign with medical background aimed at creating a
drug formulation. Typically, there are quick initial analytics performed on the
formulation, followed by *in vitro* experiments followed by mouse *in vivo* experiments.
Without the ability to use partial measurements, you would have to wait until the slow
mouse experiment for a given recommendation is measured until you could utilize any of
the other (faster) experimental outcomes for that recommendation. Furthermore, if the fast
measurements are already unpromising, the slower target measurements are possibly never
performed at all.

In BayBE, you can leverage results even if they are only partial. This is indicated
by setting the corresponding target measurement value to NaN. There are several ways to indicate this, e.g.:

* [`numpy.nan`](https://numpy.org/doc/stable/reference/constants.html#numpy.nan)
* [`pandas.NA`](https://pandas.pydata.org/docs/reference/api/pandas.NA.html#pandas.NA)
* `None`
* `float(""nan"")`

Let us consider this 3-batch of recommendations, assuming
we need to measure âTarget_1â, âTarget_2â and âTarget_3â:

```python
import numpy as np
import pandas as pd

rec = campaign.recommend(batch_size=3)
# Resetting the index to have easier access via .loc later
measurements = rec.reset_index(drop=True)

# Add measurement results
measurements.loc[0, ""Target_1""] = 10.3
measurements.loc[0, ""Target_2""] = 0.5
measurements.loc[0, ""Target_3""] = 11.1

measurements.loc[1, ""Target_1""] = 7.1
measurements.loc[1, ""Target_2""] = np.nan  # not measured yet
measurements.loc[1, ""Target_3""] = 12.2

measurements.loc[2, ""Target_1""] = 11.4
measurements.loc[2, ""Target_2""] = pd.NA  # not measured yet
measurements.loc[2, ""Target_3""] = None  # not measured yet

measurements

# Proceed with campaign.add_measurements ...
```

| Param_1   |   Param_2 | ...   |   Target_1 |   Target_2 |   Target_3 |
|-----------|-----------|-----|------------|------------|------------|
| on        |       1.1 | ...   |       10.3 |        0.5 |       11.1 |
| on        |       3.8 | ...   |        7.1 |      nan   |       12.2 |
| off       |       2.9 | ...   |       11.4 |      nan   |      nan   |

Internally, the incomplete rows are dropped when fitting a surrogate model for each
target. If you use an unsupported surrogate model, an error will be thrown at runtime.:class: important
The described method only works if the surrogate model uses a separate data basis
for each target. This is e.g. the case if you use the
[`CompositeSurrogate`](baybe.surrogates.composite.CompositeSurrogate)
to enable multi-output modeling required by the 
[`ParetoObjective`](baybe.objectives.pareto.ParetoObjective). For details, see 
[multi-output modeling](multi_output_modeling).

The [`DesirabilityObjective`](baybe.objectives.desirability.DesirabilityObjective) does 
not currently utilize multi-output models and hence does not support partial results.  ",How can you indicate a target measurement that has not been measured yet in BayBE?,why can't I  add partial result to my experiment model?
35,"# Campaigns
## Acquisition Function Values

In some cases, you may want to examine the specific acquisition function values for a given set of candidates. Campaigns provide two straightforward methods for this purpose:

- `acquisition_values()`: Computes **individual** acquisition values for each candidate in the set, answering the question  *âWhat is the expected utility of running this experiment in isolation?â*
- `joint_acquisition_value()`: Computes the **joint** acquisition value for the entire candidate batch, answering the question  *âWhat is the overall expected utility of running this batch of experimentsâ?*

```python
rec = campaign.recommend(5)
acq_values = campaign.acquisition_values(rec)  # contains 5 numbers
joint_acq_value = campaign.joint_acquisition_value(rec)  # contains 1 number
```

By default, both methods use the acquisition function of the underlying recommender. However, you can also specify a custom acquisition function if needed:

```python
from baybe.acquisition import UCB, qPSTD

acq_values = campaign.acquisition_values(rec, UCB())
joint_acq_value = campaign.joint_acquisition_value(rec, qPSTD())
```",How can I manually compute the value of my acquisition function?,What does the acquisition_values() method in BayBE campaigns return when given a set of candidates?
36,"# Campaigns
## Serialization

Like other BayBE objects, [`Campaigns`]() can be (de-)serialized
using their [`from_json`]()/
[`to_json`]() methods, which
allow to convert between Python objects and their corresponding representation in JSON
format:

```python
campaign_json = campaign.to_json()
reconstructed = Campaign.from_json(campaign_json)
assert campaign == reconstructed
```

General information on this topic can be found in our
[serialization user guide](serialization.md).
For campaigns, however, this possibility is particularly noteworthy as it enables
one of the most common workflows in this context â
persisting the current state of a campaign for long-term storage and continuing the
experimentation at a later point in time:

1. Get your campaign object
   * When initiating the workflow, create a new campaign object
   * When coming from the last step below, **deserialize** the existing campaign object
2. Add the latest measurement results
3. Get a recommendation
4. **Serialize** the campaign and store it somewhere
5. Run your (potentially lengthy) real-world experiments
6. Repeat

## Further Information

Campaigns are created as a first step in most of our
[examples]().
For more details on how to define campaigns for a specific use case, we thus propose
to have a look at the most suitable example.",How can you save and later restore the state of a BayBE campaign object?,What would be the best practice for setting up a long-term experiment with BayBE?
37,"# Campaigns

When it comes to Bayesian optimization, campaigns emerge as an essential component.
They encompass a group of interconnected experiments that collectively aim to navigate
the search space and find an optimal solution. They take center stage in orchestrating
the iterative process of selecting, evaluating, and refining candidate solutions.
Thus, campaigns are an integral part of Bayesian optimization and, accordingly,
they also play a central role in BayBE.

The [`Campaign`]() class provides a structured framework for
defining and documenting an experimentation process.
It further serves as the primary interface for interacting with BayBE as a user
since it is responsible for handling experimental data, making recommendations, adding
measurements, and most other user-related tasks.

## Creating a Campaign
### Basic Creation

Creating a campaign requires specifying at least two pieces of information that
describe the underlying optimization problem at hand:

| Campaign Specification                     | BayBE Class                                              |
|--------------------------------------------|----------------------------------------------------------|
| What should be optimized in the campaign?  | `Objective` ([class]() / [user guide](objectives.md))    |
| Which experimental factors can be altered? | `SearchSpace` ([class]() / [user guide](searchspace.md)) |

Apart from this basic configuration, it is possible to further define the specific
optimization
`Recommender`Â ([class]()
/ [user guide](recommenders.md)) to be used.

```python
from baybe import Campaign

campaign = Campaign(
    searchspace=searchspace,  # Required
    objective=objective,  # Required
    recommender=recommender,  # Optional
)
```",What are the two required components needed to create a campaign in BayBE?,I am starting a new optimization project. I know which experimental factors I can control (like ?Temperature and ?Solvent) and I know what I want to maximize (the ?Yield of a reaction). According to the documentation. How can I create a campaign?
38,"# Campaigns
## Creating a Campaign
### Creation From a JSON Config

Instead of using the default constructor, it is also possible to create a `Campaign`
from a JSON configuration string via
[`Campaign.from_config`]().
Herein, the expected JSON schema of the string should mirror the class
hierarchy of the objects nested in the corresponding campaign object.
The string can be easily validated using
[Campaign.validate_config]() without
instantiating the object, which skips the potentially costly search space creation step.
For more details and a full exemplary config, we refer to the corresponding
[example]().",Can I create a campaign from a JSON config?,How can you create a Campaign in BayBE without using the default constructor?
39,"# Campaigns
## Getting Recommendations
### Basics

To obtain a recommendation for the next batch of experiments, we can query the
campaign via the [`recommend`]() method.
It expects a parameter `batch_size` that specifies the desired number of
experiments to be conducted.

```python
rec = campaign.recommend(batch_size=3)
```

Calling the function returns a `DataFrame` with `batch_size` many rows, each
representing a particular parameter configuration from the campaignâs search space.
Thus, the following might be a `DataFrame` returned by `recommend` in a search space
with the three parameters `Categorical_1`, `Categorical_2` and `Num_disc_1`:

|    | Categorical_1   | Categorical_2   |   Num_disc_1 |
|----|-----------------|-----------------|--------------|
| 15 | B               | good            |            1 |
| 18 | C               | bad             |            1 |
|  9 | B               | bad             |            1 |:class: important
In general, the parameter configurations in a recommended batch are **jointly**
optimized and therefore tailored to the specific batch size requested. 
This means that for two batches of different requested sizes, the smaller batch will not 
necessarily correspond to a subset of the configurations contained in the larger batch. 
An intuitive explanation for this phenomenon is that the more experiments one can 
afford to run, the less need there is to focus on ""safe bets"" and the more room
becomes available to test ""high-risk/high-gain"" configurations, since only one of the
tested configurations from the batch has to perform well.

**The bottom line is:** You should always ask for exactly as many
recommendations as you are willing to run parallel experiments in your next 
experimental iteration.
An approach where only a subset of experiments taken from a larger recommended batch is
used is strongly discouraged.

**Note:** While the above distinction is true in the general case, it may not be 
relevant for all configured settings, for instance, when the used recommender 
is not capable of joint optimization. Currently, the 
[BotorchRecommender](baybe.recommenders.pure.bayesian.botorch.BotorchRecommender)
is the only recommender available that performs joint optimization.:class: note
If you have a fixed experimental budget but the luxury of choosing 
whether to run your experiments sequentially or in parallel, sequential 
experimentation will give you the better overall results in expectation.
This is because in the sequential approach, each subsequent recommendation can 
leverage the additional data from previous iterations, which allows 
more accurate predictive models to be built. However, in real-world use cases, the 
question is typically answered by other factors, such as whether parallel
experimentation is feasible in the first place, or whether the given time budget 
even allows for sequential runs.",Can I just run part of the recommend batch without harming my model?,Which recommender in BayBE is currently capable of joint optimization for batch recommendations?
40,"# Campaigns
## Getting Recommendations
### Candidate Control in Discrete Spaces

For discrete search spaces, campaigns provide additional control over how the candidate
set of recommendable points is built based on the trajectory the campaign has taken so
far. This is done by setting the following Boolean flags:

- `allow_recommending_already_measured`:  Controls whether points that have already been
  measured can be recommended.
- `allow_recommending_already_recommended`: Controls whether previously recommended points can
  be recommended again.
- `allow_recommending_pending_experiments`: Controls whether points marked as
  `pending_experiments` can be recommended (see [asynchronous
  workflows](async.md#pending-experiments)). # Campaigns
## Getting Recommendations
### Caching of Recommendations

The `Campaign` object caches the last batch of recommendations returned, in order to
avoid unnecessary computations for subsequent queries between which the status
of the campaign has not changed.
The cache is invalidated as soon as new measurements are added or a different
batch size is desired.
The latter is necessary because each batch is optimized for the specific number of
experiments requested (see note above).",What causes the recommendation cache in a BayBE Campaign to be invalidated?,How do I specify that points can be recommended again (in subsequent experiments) when previous experiments are still pending?
41,"# Campaigns
## Adding Measurements

Available experimental data can be added at any time during the campaign lifecycle using
the [`add_measurements`]() method,
which expects a `DataFrame` containing the values of the used experimental parameters
and all corresponding target measurements.
If measurements are to be added immediately after a call to `recommend`,
this is most easily achieved by augmenting the  `DataFrame` returned from that call
with the respective target columns.

```python
rec[""Target_max""] = [2, 4, 9]  # 3 values matching the batch_size of 3
campaign.add_measurements(rec)
new_rec = campaign.recommend(batch_size=5)
```

After adding the measurements, the corresponding `DataFrame` thus has the following
form:

|    | Categorical_1   | Categorical_2   |   Num_disc_1 |   Target_max |
|----|-----------------|-----------------|--------------|--------------|
| 15 | B               | good            |            1 |            2 |
| 18 | C               | bad             |            1 |            4 |
|  9 | B               | bad             |            1 |            9 |",What method is used to add experimental data during a BayBE campaign?,Are there any restrictions on the parameter values that I can add via the `add_measurements` method?
42,"# Constraints

Experimental campaigns often have naturally arising constraints on the parameters and
their combinations. Such constraints could for example be:

* When optimizing a mixture, the relative concentrations of the used ingredients must
  add up to 1.0.
* For chemical reactions, a reagent might be incompatible with high temperatures, hence
  these combinations must be excluded.
* Certain settings are dependent on other parameters, e.g. a set of parameters only
  becomes relevant if another parameter called `""Switch""` has the value `""on""`.

Similar to parameters, BayBE distinguishes two families of constraints, derived from the
abstract [`Constraint`]() class: discrete and
continuous constraints ([`DiscreteConstraint`](),
[`ContinuousConstraint`]()).
A constraint is called discrete/continuous if it operates on a set of exclusively
discrete/continuous parameters.       
 #### WARNING
:class: note
Currently, BayBE does not support hybrid constraints, that is, constraints which
operate on a mixed set of discrete and continuous parameters. If such a constraint is
necessary, it is possible to rephrase the parametrization so that the parameter set
is exclusively discrete or continuous in most cases.",Does BayBE support hybrid constraints that operate on both discrete and continuous parameters?,How can I make sure that all my experimental constraints are reflected in the campaign set-up?
43,"# Constraints
## Discrete Constraints
### DiscreteSumConstraint and DiscreteProductConstraint

[`DiscreteSumConstraint`]()
and [`DiscreteProductConstraint`]()
impose conditions on sums or products of numerical parameters.
[In the first example from `ContinuousLinearConstraint`](#clc), we
had three continuous parameters `x_1`, `x_2` and `x_3`, which needed to sum
up to 1.0.
If these parameters were instead discrete, the corresponding constraint would look like:

```python
from baybe.constraints import DiscreteSumConstraint, ThresholdCondition

DiscreteSumConstraint(
    parameters=[""x_1"", ""x_2"", ""x_3""],
    condition=ThresholdCondition(  # set condition that should apply to the sum
        threshold=1.0,
        operator=""="",
        tolerance=0.001,  # optional; here, everything between 0.999 and 1.001 would also be considered valid
    ),
)
```

An end to end example can be found [here]().",How do I implement variable constraints that work for noisy data?,What does DiscreteSumConstraint do in BayBE?
44,"# Constraints
## Discrete Constraints
### DiscreteNoLabelDuplicatesConstraint

Sometimes, duplicated labels in several parameters are undesirable.
Consider an example with two solvents that describe different mixture
components.
These might have the exact same or overlapping sets of possible values, e.g.
`[""Water"", ""THF"", ""Octanol""]`.
It would not necessarily be reasonable to allow values in which both solvents show the
same label/component.
We can exclude such occurrences with the
[`DiscreteNoLabelDuplicatesConstraint`]():

```python
from baybe.constraints import DiscreteNoLabelDuplicatesConstraint

DiscreteNoLabelDuplicatesConstraint(parameters=[""Solvent_1"", ""Solvent_2""])
```

Without this constraint, combinations like below would be possible:

|    | Solvent_1   | Solvent_2   | With DiscreteNoLabelDuplicatesConstraint   |
|----|-------------|-------------|--------------------------------------------|
|  1 | Water       | Water       | would be excluded                          |
|  2 | THF         | Water       |                                            |
|  3 | Octanol     | Octanol     | would be excluded                          |

The usage of `DiscreteNoLabelDuplicatesConstraint` is part of the
[example on slot-based mixtures]().",What does the DiscreteNoLabelDuplicatesConstraint prevent in BayBE experiments?,How do I avoid duplicated labels in several parameters?
45,"# Constraints
## Discrete Constraints
### DiscreteLinkedParametersConstraint

The [`DiscreteLinkedParametersConstraint`]()
is, in a sense, the opposite of the
[`DiscreteNoLabelDuplicatesConstraint`]().
It will ensure that **only** entries with duplicated labels are present.
This can be useful, for instance, in situations where we have one parameter but would
like to include it with several encodings:

```python
from baybe.parameters import SubstanceParameter
from baybe.constraints import DiscreteLinkedParametersConstraint

dict_solvents = {""Water"": ""O"", ""THF"": ""C1CCOC1"", ""Octanol"": ""CCCCCCCCO""}
solvent_encoding1 = SubstanceParameter(
    name=""Solvent_RDKIT_enc"",
    data=dict_solvents,
    encoding=""RDKIT"",
)
solvent_encoding2 = SubstanceParameter(
    name=""Solvent_MORDRED_enc"",
    data=dict_solvents,
    encoding=""MORDRED"",
)
DiscreteLinkedParametersConstraint(
    parameters=[""Solvent_RDKIT_enc"", ""Solvent_MORDRED_enc""]
)
```

|    | Solvent_RDKIT_enc   | Solvent_MORDRED_enc   | With DiscreteLinkedParametersConstraint   |
|----|---------------------|-----------------------|-------------------------------------------|
|  1 | Water               | Water                 |                                           |
|  2 | THF                 | Water                 | would be excluded                         |
|  3 | Octanol             | Octanol               |                                           |",How do I keep only parameters which have duplicate values across my dataset?,What does the DiscreteLinkedParametersConstraint ensure in BayBE?
46,"# Constraints
## Discrete Constraints
### DiscreteDependenciesConstraint

A dependency is a situation where parameters depend on other parameters.
Letâs say an experimental setup has a parameter called `""Switch""`, which turns on
pieces of equipment that are optional.
This means the other parameters (called `affected_parameters`) are only relevant if
the switch parameter has the value `""on""`. If the switch is `""off""`, the affected
parameters are irrelevant.

You can specify such a dependency with the
[`DiscreteDependenciesConstraint`]()
, which requires:

1. A list `parameters` with the names of the parameters upon which others depend.
2. A list `conditions`, specifying the values of the corresponding entries in
   `parameters` that âactivateâ the dependent parameters.
3. A list of lists, each containing the `affected_parameters`, which become relevant
   only if the corresponding entry in `parameters` is active as specified by the
   entry in `conditions`.

Internally, BayBE drops elements from the `SearchSpace` where affected parameters are
irrelevant. Since in our example `""off""` is still a valid value for the switch, the
`SearchSpace` will still retain **one** configuration for that setting, showing arbitrary
values for the `affected_parameters` (which can be ignored).

<a id=""ddc""></a>

#### IMPORTANT
BayBE requires that all dependencies are declared in a single
`DiscreteDependenciesConstraint`. Creating a `SearchSpace` from multiple
`DiscreteDependenciesConstraint`âs will throw a validation error.

In the example below, we mimic a situation where there are two switches and each switch
activates two other parameters that are only relevant if the first switch is `""on""` / the
second switch is set to `""right""`, respectively.

```python
from baybe.constraints import DiscreteDependenciesConstraint, SubSelectionCondition

DiscreteDependenciesConstraint(
    parameters=[""Switch_1"", ""Switch_2""],  # the two parameters upon which others depend
    conditions=[
        SubSelectionCondition(
            # values of Switch_1 that activate the affected parameters
            selection=[""on""]
        ),
        SubSelectionCondition(
            # values of Switch_2 that activate the affected parameters
            selection=[""right""]
        ),
    ],
    affected_parameters=[
        [""Solvent"", ""Fraction""],  # parameters affected by Switch_1
        [""Frame_1"", ""Frame_2""],  # parameters affected by Switch_2
    ],
)
```
",What happens if you create a SearchSpace from multiple DiscreteDependenciesConstraint objects in BayBE?,Can I create  a `SearchSpace` from multiple`DiscreteDependenciesConstraint`?
47,"# Constraints
## Discrete Constraints
### DiscretePermutationInvarianceConstraint

Permutation invariance, enabled by the
[`DiscretePermutationInvarianceConstraint`]()
, is a property where combinations of values of multiple
parameters do not depend on their order due to some symmetry in the experiment.
Suppose we create a mixture containing up to three solvents, i.e. parameters
âSolvent_1â, âSolvent_2â, âSolvent_3â.
In this situation, all combinations from the following table would be equivalent,
hence the `SearchSpace` should effectively only contain one of them.

|    | Solvent_1    | Solvent_2    | Solvent_3    |
|----|--------------|--------------|--------------|
|  1 | Substance_43 | Substance_3  | Substance_12 |
|  2 | Substance_43 | Substance_12 | Substance_3  |
|  3 | Substance_3  | Substance_12 | Substance_43 |
|  4 | Substance_3  | Substance_43 | Substance_12 |
|  5 | Substance_12 | Substance_43 | Substance_3  |
|  6 | Substance_12 | Substance_3  | Substance_43 |

#### NOTE
Complex properties such as permutation invariance not only affect the search space but
should ideally also constrain the surrogate model. For instance, the kernels in a
Gaussian process can be made permutation-invariant to reflect this constraint, which
generally results in a better learning curve. Note that at this stage no
surrogate model provided by BayBE takes care of these invariances. This means the
invariance is ignored during model fitting and these models do not benefit
from a priori known constraints and invariances between parameters. However, generally,
the optimization will still work. We are in the process of enabling this as new feature,
but in the meantime the user can introduce their own
[custom surrogate model]()
to include these.

Letâs add to the mixture example the fact that not only the choice of substance but also
their relative mixture fractions are parameters, i.e. âFraction_1â, âFraction_2â and
âFraction_3â.
This also implies that the solvent parameters depend on their corresponding
fraction being `> 0.0`, because in the case `== 0.0` the choice of solvent is
irrelevant. This models a scenario that allows âup to, but not necessarily,
three solventsâ.

#### IMPORTANT
If some of the `parameters` of the `DiscretePermutationInvarianceConstraint` are
dependent on other parameters, we require that the dependencies are provided as a
`DiscreteDependenciesConstraint` to the `dependencies` argument of the
`DiscretePermutationInvarianceConstraint`. This
`DiscreteDependenciesConstraint` will not count towards the maximum limit of one
`DiscreteDependenciesConstraint` discussed [here](#ddc).

The `DiscretePermutationInvarianceConstraint` below applies to our example and
removes permutation-invariant combinations of solvents that have additional
dependencies as well:

```python
from baybe.constraints import (
    DiscretePermutationInvarianceConstraint,
    DiscreteDependenciesConstraint,
    ThresholdCondition,
)

DiscretePermutationInvarianceConstraint(
    parameters=[""Solvent_1"", ""Solvent_2"", ""Solvent_3""],
    # `dependencies` is optional; it is only required if some of the permutation
    # invariant entries in `parameters` have dependencies on other parameters
    dependencies=DiscreteDependenciesConstraint(
        parameters=[""Fraction_1"", ""Fraction_2"", ""Fraction_3""],
        conditions=[
            ThresholdCondition(threshold=0.0, operator="">""),
            ThresholdCondition(threshold=0.0, operator="">""),
            ThresholdCondition(threshold=0.0, operator="">""),
        ],
        affected_parameters=[[""Solvent_1""], [""Solvent_2""], [""Solvent_3""]],
    ),
)
```
","I already have a DiscreteDependenciesConstraint in my model, can I use another DiscreteDependenciesConstraint inside  DiscretePermutationInvarianceConstraint?",What does the DiscretePermutationInvarianceConstraint do in BayBE?
48,"# Constraints
## Discrete Constraints
### DiscreteCardinalityConstraint

Like its [continuous cousin](), the
`DiscreteCardinalityConstraint` lets you control the
number of active parameters in your design. The construction works analogously:

```python
from baybe.constraints import DiscreteCardinalityConstraint

DiscreteCardinalityConstraint(
    parameters=[""Fraction_1"", ""Fraction_2"", ""Fraction_3""],
    min_cardinality=1,  # defaults to 0
    max_cardinality=2,  # defaults to the number of affected parameters (here: 3)
)
```",What does the DiscreteCardinalityConstraint control in a BayBE design?,What are the keywords for setting the minimum and maximum cardinality when using the `DiscreteCardinalityConstraint`?
49,"# Constraints
## Discrete Constraints
### DiscreteCustomConstraint

With a [`DiscreteCustomConstraint`]()
constraint, you can specify a completely custom filter:

```python
import pandas as pd
import numpy as np
from baybe.constraints import DiscreteCustomConstraint


def custom_filter(df: pd.DataFrame) -> pd.Series:  # this signature is required
    """"""
    In this example, we exclude entries where the square root of the
    temperature times the cubed pressure are larger than 5.6.
    """"""
    mask_good = np.sqrt(df[""Temperature""]) * np.power(df[""Pressure""], 3) <= 5.6

    return mask_good


DiscreteCustomConstraint(
    parameters=[  # the custom function will have access to these variables
        ""Pressure"",
        ""Temperature"",
    ],
    validator=custom_filter,
)
```

Find a detailed example [here]().

#### WARNING
Due to the arbitrary nature of code and dependencies that can be used in the
`DiscreteCustomConstraint`, (de-)serializability cannot be guaranteed. As a consequence,
using a `DiscreteCustomConstraint` results in an error if you attempt to serialize
the corresponding object or higher-level objects containing it.",What happens if you try to serialize a BayBE DiscreteCustomConstraint object?,Is it possible to use serialization in a use case with custom constraints?
50,"# Constraints
## Continuous Constraints

#### WARNING
Not all surrogate models are able to treat continuous constraints. In such situations
the constraints are currently silently ignored.

### ContinuousLinearConstraint

The [`ContinuousLinearConstraint`]()
asserts that the following kind of equations are true (up to numerical rounding errors):

$$

\sum_{i} x_i \cdot c_i = \text{rhs} \\
\sum_{i} x_i \cdot c_i >= \text{rhs} \\
\sum_{i} x_i \cdot c_i <= \text{rhs}
$$

where $x_i$ is the value of the $i$âth parameter affected by the constraint,
$c_i$ is the coefficient for that parameter, and $\text{rhs}$ is a user-chosen number.
The (in)equality type is defined by the `operator` keyword.

As an example, letâs assume we have three parameters named `x_1`, `x_2` and
`x_3`, which describe the relative concentrations in a mixture campaign.
The constraint assuring that they always sum up to 1.0 would look like this:

```python
from baybe.constraints import ContinuousLinearConstraint

ContinuousLinearConstraint(
    parameters=[""x_1"", ""x_2"", ""x_3""],  # these parameters must exist in the search space
    operator=""="",
    coefficients=[1.0, 1.0, 1.0],
    rhs=1.0,
)
```

Let us amend the example from above and assume that there is always a fourth component
to the mixture that serves as a âfillerâ. In such a case, we might want to ensure that
the first three components only make up to 80% of the mixture.
The following constraint would achieve this:

```python
from baybe.constraints import ContinuousLinearConstraint

ContinuousLinearConstraint(
    parameters=[""x_1"", ""x_2"", ""x_3""],
    operator=""<="",
    coefficients=[1.0, 1.0, 1.0],
    rhs=0.8,
)
```

A more detailed example can be found
[here]().",I feel like my constraints have been ignored. Can that happen?,How do you specify a constraint in BayBE to ensure that three parameters sum up to 1.0?
51,"# Constraints
## Continuous Constraints
### ContinuousCardinalityConstraint

The `ContinuousCardinalityConstraint` gives you a
tool to control the number of active factors (i.e. parameters that take a non-zero
value) in your designs. This comes handy, for example, when designing mixtures with a
limited number of components.

To create a constraint of this kind, simply specify the set of parameters on which the
constraint is to be imposed, together with the corresponding upper and lower cardinality
limits. For instance, the following constraint would ensure that there is always a
minimum of one and a maximum of two components in each parameter configuration:

```python
from baybe.constraints import ContinuousCardinalityConstraint

ContinuousCardinalityConstraint(
    parameters=[""x_1"", ""x_2"", ""x_3""],
    min_cardinality=1,  # defaults to 0
    max_cardinality=2,  # defaults to the number of affected parameters (here: 3)
    relative_threshold=0.001,  # optional, defines the range of values considered active
)
```",I want to optimize mixtures.,What does the ContinuousCardinalityConstraint control in BayBE?
52,"# Constraints
## Conditions

Conditions are elements used within discrete constraints.
While discrete constraints can operate on one or multiple parameters, a condition
always describes the relation of a single parameter to its possible values.
It is through chaining several conditions in constraints that we can build complex
logical expressions for them.
### ThresholdCondition

For numerical parameters, we might want to select a certain range, which can be
achieved with a [`ThresholdCondition`]():

```python
from baybe.constraints import ThresholdCondition

ThresholdCondition(  # will select all values above 150
    threshold=150,
    operator="">"",
)
```  ### SubSelectionCondition

In case a specific subset of values needs to be selected, it can be done with the
[`SubSelectionCondition`]():

```python
from baybe.constraints import SubSelectionCondition

SubSelectionCondition(  # will select two solvents identified by their labels
    selection=[""Ethanol"", ""DMF""]
)
```",I want to constrain the temperature parameter of my experiment,What is the purpose of ThresholdCondition in BayBE constraints?
53,"# Constraints
## Discrete Constraints

Discrete constraints currently do not affect the optimization process directly.
Instead, they act as a filter on the search space.
For instance, a search space created via [`from_product`]()
might include invalid combinations, which can be removed again by applying constraints.

Discrete constraints have in common that they operate on one or more parameters,
identified by the `parameters` member, which expects a list of parameter names as
strings.
All of these parameters must be present in the search space specification.### DiscreteExcludeConstraint

The [`DiscreteExcludeConstraint`]()
constraint simply removes a set of search space elements, according to its
specifications.

The following example would exclude entries where âEthanolâ and âDMFâ are combined with
temperatures above 150, which might be due to their chemical instability at those
temperatures:

```python
from baybe.constraints import (
    DiscreteExcludeConstraint,
    ThresholdCondition,
    SubSelectionCondition,
)

DiscreteExcludeConstraint(
    parameters=[""Temperature"", ""Solvent""],  # names of the affected parameters
    combiner=""AND"",  # specifies how the conditions are logically combined
    conditions=[  # requires one condition for each entry in parameters
        ThresholdCondition(threshold=150, operator="">""),
        SubSelectionCondition(selection=[""Ethanol"", ""DMF""]),
    ],
)
```

A more detailed example can be found
[here]().",What does the DiscreteExcludeConstraint do in BayBE?,How do I create DiscreteExcludeConstraint?
54,"# Environment Variables

Several aspects of BayBE can be configured via environment variables.

## Basic Instructions

Setting an environment variable with the name `ENVVAR_NAME` is best done before calling
any Python code, and must also be done in the same session unless made persistent, e.g.
via `.bashrc` or similar:

```bash
ENVAR_NAME=""my_value""
python do_baybe_work.py
```

Or on Windows:

```shell
set ENVAR_NAME=my_value
```

Note that variables set in this manner are interpreted as text, but converted internally
to the needed format. See for instance the [`strtobool`]()
converter for values that can be set so BayBE can interpret them as Booleans.

It is also possible to set environment variables in Python:

```python
import os

os.environ[""ENVAR_NAME""] = ""my_value""

# proceed with BayBE code ...
```

However, this needs to be done carefully at the entry point of your script or session and
will not persist between sessions.",How can I set an environment variable for BayBE on Windows?,Can I use env vars?
55,"# Environment Variables
## Telemetry

Monitored quantities:

* `batch_size` used when querying recommendations
* Number of parameters in the search space
* Number of constraints in the search space
* How often [`recommend`]() was called
* How often [`add_measurements`]() was called
* How often a search space is newly created
* How often initial measurements are added before recommendations were calculated
  (ânaked initial measurementsâ)
* The fraction of measurements added that correspond to previous recommendations
* Each measurement is associated with a truncated hash of the user- and hostname

The following environment variables control the behavior of BayBE telemetry:

- `BAYBE_TELEMETRY_ENABLED`: Flag that can turn off telemetry entirely (default is
  `True`). To turn it off set it to `False`.
- `BAYBE_TELEMETRY_ENDPOINT`: The receiving endpoint URL for telemetry data.
- `BAYBE_TELEMETRY_VPN_CHECK`: Flag turning an initial telemetry connectivity check
  on/off (default is `True`).
- `BAYBE_TELEMETRY_VPN_CHECK_TIMEOUT`: The timeout in seconds for the check whether the
  endpoint URL is reachable.
- `BAYBE_TELEMETRY_USERNAME`: The name of the user executing BayBE code. Defaults to a
  truncated hash of the username according to the OS.
- `BAYBE_TELEMETRY_HOSTNAME`: The name of the machine executing BayBE code. Defaults to
  a truncated hash of the machine name.",Is it possible to fully enumerate a search space combinatorially?,What environment variable should be set to False to completely disable BayBE telemetry?
56,"# Environment Variables
## Polars

If BayBE was installed with the additional `polars` dependency (`baybe[polars]`), it
will use the advanced methods of Polars to create the searchspace lazily and perform a
streamed evaluation of constraints. This will improve speed and memory consumption
during this process, and thus might be beneficial for very large search spaces.

Since this is still somewhat experimental, you might want to deactivate Polars without
changing the Python environment. To do so, you can set the environment variable
`BAYBE_DEACTIVATE_POLARS` to any truthy value accepted by
[`strtobool`]().:class: caution

For performance reasons, search space manipulation using `polars` is not
guaranteed to produce the same row order as the corresponding `pandas` operations.",How can I deactivate the use of Polars in BayBE without changing the Python environment?,What does Polars do?
57,"# Environment Variables
## Disk Caching

For some components, such as the
[`SubstanceParameter`](), some of the
computation results are cached in local storage.

By default, BayBE determines the location of temporary files on your system and puts
cached data into a subfolder `.baybe_cache` there. If you want to change the location of
the disk cache, change:

```bash
BAYBE_CACHE_DIR=""/path/to/your/desired/cache/folder""
```

By setting

```bash
BAYBE_CACHE_DIR=""""
```

you can turn off disk caching entirely.",Where is my cache folder?,How can I disable disk caching in BayBE?
58,"# Environment Variables
## EXPERIMENTAL: Floating Point Precision

In general, double precision is recommended because numerical stability during optimization
can be bad when single precision is used. This impacts gradient-based optimization,
i.e. search spaces with continuous parameters, more than optimization without gradients.

If you still want to use single precision, you can set the following Boolean variables:

- `BAYBE_NUMPY_USE_SINGLE_PRECISION` (defaults to `False`)
- `BAYBE_TORCH_USE_SINGLE_PRECISION` (defaults to `False`):class: warning
Currently, it cannot be guaranteed that all calculations will be performed in single precision,
even when setting the aforementioned variables. The reason is that there are several code snippets
within `BoTorch` that transform single precision variables to double precision variables.
Consequently, this feature is currently only available as an *experimental* feature.
We are however actively working on fully enabling single precision.",My optimization is running slowly and using too much memory. How do I set the ?BAYBE_NUMPY_USE_SINGLE_PRECISION and ?BAYBE_TORCH_USE_SINGLE_PRECISION environment variables to try and speed it up?,How can I enable single precision in BayBE for NumPy and PyTorch?
59,"# Getting Recommendations

The core functionality of BayBE is its ability to generate context-aware recommendations
for your experiments. This page covers the basics of the corresponding user interface,
assuming that a `SearchSpace` object and optional
`Objective` and measurement objects are already in place

## The `recommend` Call

BayBE offers two entry points for requesting recommendations:

* <a id=""stateless""></a>

  **Recommenders**<br />
  \\\\
  If a single (batch) recommendation is all you need, the most direct way to interact is
  to ask one of BayBEâs recommenders for it, by calling its
  `recommend()` method. To do so,
  simply pass all context information to the method call. This way, you interact with
  BayBE in a completely *stateless* way since all relevant components are explicitly
  provided at call time.

  For example, using the `BotorchRecommender`:
  ```python
  recommender = BotorchRecommender()
  recommendation = recommender.recommend(batch_size, searchspace, objective, measurements)
  ```
* <a id=""stateful""></a>

  **Campaigns**<br />
  \\\\
  By contrast, if you plan to run an extended series of experiments where you feed newly
  arriving measurements back to BayBE and ask for a refined experimental design,
  creating a `Campaign` object that tracks the experimentation
  progress is a better choice. This offers *stateful* way of interaction where
  the context is fully maintained by the campaign object:
  ```python
  recommender = BotorchRecommender()
  campaign = Campaign(searchspace, objective, recommender)
  campaign.add_measurements(measurements)
  recommendation = campaign.recommend(batch_size)
  ```","What is the most direct, stateless way to get a recommendation from BayBE for a single batch experiment?",How do I get my next experiment?
60,"# Getting Recommendations
## Excluding Configurations

When asking for recommendation, you often donât want to consider all possible
combinations of parameter values (a.k.a. the full Cartesian product space) but you may
want to exclude certain configurations that are known to be infeasible or undesirable.
There are several ways to do this, including using BayBEâs sophisticated [constraint
machinery](constraints.md). Which approach is the right choice for you depends on
whether you want to exclude configurations *permanently* or (in-)activate them
*dynamically* during your experimentation cycle.",How can I exclude certain parameter configurations when getting recommendations in BayBE?,Can I put constraints on the recommendation space?
61,"# Getting Recommendations
## Excluding Configurations
### Permanent Exclusion

Permanently excluding certain parameter configurations from the recommendation is
generally done by adjusting the `SearchSpace` object
accordingly, which defines the set of candidate configurations that will be considered.

BayBE provides several ways to achieve this, which weâll illustrate by comparing against
the following âfullâ search space:

```python
searchspace_full = TaskParameter(""p"", [""A"", ""B"", ""C""]).to_searchspace()
```

Depending on the specific needs and complexity of the filtering operation, one approach
may be preferred over the other, but generally these mechanisms exist:

* Restricting individual parameter objects via `active_values`:
  ```python
  searchspace_reduced = TaskParameter(
      ""p"", [""A"", ""B"", ""C""], active_values=[""A"", ""B""]
  ).to_searchspace()
  ```

  This is possible for all [label-like parameters](parameters.md#label-like). :class: caution

Note that this is *not* the same as defining the parameter with a reduced set of
values `[""A"", ""B""]` since in this case the value ""C"" would be undefined. This
makes adding measurements containing that value impossible. 
* Specifying only a subset of configurations (discrete spaces only):
  ```python
  searchspace_reduced = SearchSpace.from_dataframe(
      pd.DataFrame({""p"": [""A"", ""B""]}),
      parameters=[TaskParameter(""p"", [""A"", ""B"", ""C""])],
  )
  ```
* Filtering the search space using constraints:
  ```python
  searchspace_reduced = SearchSpace.from_product(
      parameters=[CategoricalParameter(""p"", [""A"", ""B"", ""C""])],
      constraints=[DiscreteExcludeConstraint([""p""], [SubSelectionCondition([""C""])])],
  )
  ```
* Using specialized constructors like
  `from_simplex()`.",Is it possible to restrict the search space to avoid certain combinations or parts of the space?,How can you permanently exclude certain parameter configurations from recommendations in BayBE?
62,"# Getting Recommendations
## Excluding Configurations
### Dynamic Exclusion

Dynamic exclusion of candidates means to in-/exclude certain parameter configurations
while you are already in the middle of your experimentation process. Here,
we need to consider two different cases:

* **Recommenders**<br />
  \\\\
  Since recommender queries are [stateless]() with respect to the
  experimental context, you can easily adjust your search space object for each query
  as needed using any of the *permanent* exclusion methods. For example:
  ```python
  # Recommendation with full search space
  searchspace_full = CategoricalParameter(""p"", [""A"", ""B"", ""C""]).to_searchspace()
  recommender.recommend(batch_size, searchspace_full, objective, measurements)

  # Recommendation with reduced search space
  searchspace_reduced = TaskParameter(
      ""p"", [""A"", ""B"", ""C""], active_values=[""A"", ""B""]
  ).to_searchspace()
  recommender.recommend(batch_size, searchspace_reduced, objective, measurements)
  ```
* **Campaigns**<br />
  \\\\
  Because the search space must be defined before a
  `Campaign` object can be created, a different approach is
  required for [stateful queries](). For this purpose,
  `Campaign`s provide a
  `toggle_discrete_candidates()` method that allows to
  dynamically enable or disable specific candidates while the campaign is running.
  The above example thus translates to:
  ```python
  campaign = Campaign(searchspace_full, objective, measurements)
  campaign.add_measurements(measurements)

  # Recommendation with full search space
  campaign.recommend(batch_size)

  # Exclude *matching* rows
  campaign.toggle_discrete_candidates(
      pd.DataFrame({""p"": [""C""]}),
      exclude=True,
  )
  # Alternatively: Exclude *non-matching* rows
  campaign.toggle_discrete_candidates(
      pd.DataFrame({""p"": [""A"", ""B""]}),
      complement=True,
      exclude=True,
  )

  # Recommend from reduced search space using altered candidate set
  campaign.recommend(batch_size)
  ```

  Note that you can alternatively toggle candidates by passing the appropriate
  `DiscreteConstraint` objects.
  For more details, see `toggle_discrete_candidates()`.:class: attention

Currently, dynamic exclusion via toggling is only possible for discrete candidates.
To restrict the set of continuous candidates, use
{class}`~baybe.constraints.base.ContinuousConstraint`s when creating the space.:class: seealso

{class}`~baybe.campaign.Campaign`s allow you to further control the candidate
generation based on the experimental trajectory taken via their `allow_*` 
{ref}`flags <userguide/campaigns:Candidate Control in Discrete Spaces>`.",How can you dynamically exclude specific candidates during a running Campaign in BayBE?,Why did I get an error when using toggle_discrete_candidates?
63,"# Insights
In BayBE, insights provide a way of analyzing your experimental results beyond what is
required for the basic measure-recommend loop. Dependencies needed for insights are
optional and available by installing `baybe` with the respective dependency group, e.g.
via `pip install baybe[insights]`.

## Parameter Importance via SHAP

[**SH**apley **A**dditive ex**P**lanations](https://shap.readthedocs.io/en/latest/index.html)
are a popular way of interpreting models to gain insight into the importance of the
features utilized. In the context of Bayesian optimization (BO), this enables analyzing
the importance of the parameters spanning the search space. This can be useful
for identifying which parameters play a key role and which do not â learnings that can
be applied in designing future campaigns. The interface is provided by the
[`SHAPInsight`]() class.",Can BayBE explain the recommendations?,What class does BayBE provide for analyzing parameter importance via SHAP?
64,"# Insights
## Parameter Importance via SHAP
### Basic Usage

A [`SHAPInsight`]() can be obtained in several ways:

- From a [`Campaign`]() via
  [`from_campaign`]():
  ```python
  insight = SHAPInsight.from_campaign(campaign)
  ```
- From a surrogate model via [`from_surrogate`]():
  ```python
  insight = SHAPInsight.from_surrogate(surrogate, data)
  ```
- From a recommender that has an underlying surrogate model and implements
  [`get_surrogate`]()
  via [`from_recommender`]():
  ```python
  insight = SHAPInsight.from_recommender(recommender, searchspace, objective, data)
  ```

In these examples, `data` is the background data used to build the underlying explainer
model. Typically, you would set this to the measurements obtained during your
experimental campaign (for instance, [`from_campaign`]()
automatically extracts the `measurements` from the `campaign` object).",How can you obtain a SHAPInsight from a Campaign in BayBE?,How can I compute parameter importance?
65,"# Insights
## Parameter Importance via SHAP
### Plots

After creating the insight, various methods are available to visualize the results via
the [.plot]()
interface, please refer to [available SHAP plots]().

```python
insight.plot(""bar"")
```

![SHAP_Bar_Exp_Rep](_static/insights/shap_bar_exp_rep.svg)

This result agrees well with the chemical intuition that ligands are the most important
reactants to activate the conversion, resulting in higher yields.

Such plots can also be created for data sets other than the background data that
was used to generate the insight. If this is desired, pass your data frame as second
argument:

```python
insight.plot(""beeswarm"", new_measurements)
```

![SHAP_Beeswarm_Exp_Rep](_static/insights/shap_beeswarm_exp_rep.svg)

The `force` plot type requires the user to additionally select which single data point
they want to visualize by specifying the corresponding `explanation_index`:

```python
insight.plot(
    ""force"", explanation_index=3
)  # plots the force analysis of the measurement at positional index 3
```

![SHAP_Force](_static/insights/shap_force.svg)",How do you visualize the force analysis of a specific measurement using BayBE's SHAP insights?,What explainability methods are available in BayBE?
66,"# Insights
## Parameter Importance via SHAP
### Explainers

In general, SHAP is an exhaustive method testing all combinations of features. This
exhaustive algorithm (implemented by the [`shap.ExactExplainer`](https://shap.readthedocs.io/en/stable/generated/shap.ExactExplainer.html#shap.ExactExplainer) class) is
often not feasible in practice, and various approximate variants are available (see
[supported explainers]()). For details about their inner
mechanics, we refer to the [SHAP documentation](https://shap.readthedocs.io/en/latest/api.html#explainers).

The explainer can be changed when creating the insight:

```python
insight = SHAPInsight.from_campaign(
    campaign, explainer_cls=""KernelExplainer""
)  # default explainer
```",What is the default explainer class used when creating a SHAPInsight in BayBE?,I want to interpret the results BayBE provided. What do I do?
67,"# Insights
## Parameter Importance via SHAP
### Experimental and Computational Representations

[`SHAPInsight`]() by default analyzes the experimental
representation of the measurements, i.e. the that specifies parameter and target values
in terms of their actual (physical) quantities. This comes with certain limitations:

A feature importance study can still be performed by looking at the computational
representation of the data points, activated by the `use_comp_rep` flag. Since all
entries in this representation are numeric by construction, there are no limitations on
the explainer type used. A study of the computational representation might also be
useful if a deeper analysis of descriptors used is of interest to the user. In general,
for each non-numerical parameter in the experimental representation, there will be
several descriptors the computational representation:

```python
insight = SHAPInsight.from_campaign(campaign, use_comp_rep=True)
insight.plot(""bar"")
```

![SHAP_Bar_Comp_Rep](_static/insights/shap_bar_comp_rep.svg)

In addition to SHAP-based explainers, we also support
[LIME](https://arxiv.org/abs/1602.04938) and
[MAPLE](https://papers.nips.cc/paper_files/paper/2018/hash/b495ce63ede0f4efc9eec62cb947c162-Abstract.html)
variants. For example:

```python
insight = SHAPInsight.from_campaign(
    campaign, explainer_cls=""LimeTabular"", use_comp_rep=True
)
insight.plot(""bar"")
```

![SHAP_Bar_Lime](_static/insights/shap_bar_lime.svg)

As expected, the result from [`LimeTabular`](https://shap.readthedocs.io/en/stable/generated/shap.explainers.other.LimeTabular.html#shap.explainers.other.LimeTabular) are very
similar to the results from the SHAP [`KernelExplainer`](https://shap.readthedocs.io/en/stable/generated/shap.KernelExplainer.html#shap.KernelExplainer) because
both methods involve linear local approximations.",Is it possible to use SHAP explainer for non-numerical parameter?,What flag should be set to analyze the computational representation of data points in SHAPInsight?
69,"# Objective
## DesirabilityObjective

The [`DesirabilityObjective`]()
enables the combination of multiple targets via scalarization into a single numerical
value (commonly referred to as the *overall desirability*), a method also utilized in
classical DOE.:class: attention
Since measurements of different targets can vary arbitrarily in scale, all targets
passed to a
[`DesirabilityObjective`](baybe.objectives.desirability.DesirabilityObjective) must be
normalizable to enable meaningful combination into desirability values. This requires
that all provided targets must have `bounds` specified (see [target user
guide](/userguide/targets.md)).
If provided, the necessary normalization is taken care of automatically. 
Otherwise, an error will be thrown.

Besides the list of [`Targets`]()
to be scalarized, this objective type takes two
additional optional parameters that let us control its behavior:

* `weights`: Specifies the relative importance of the targets in the form of a
  sequence of positive numbers, one for each target considered.<br />
  \\\\
  **Note:**
  BayBE automatically normalizes the weights, so only their relative
  scales matter.
* `scalarizer`: Specifies the [scalarization function]()
  to be used for combining the normalized target values.
  The choices are `MEAN` and `GEOM_MEAN`, referring to the arithmetic and
  geometric mean, respectively.

The definitions of the `scalarizer`s are as follows, where $\{t_i\}$ enumerate the
**normalized** target measurements of single experiment and $\{w_i\}$ are the
corresponding target weights:

$$

\text{MEAN} &= \frac{1}{\sum w_i}\sum_{i} w_i \cdot t_i \\
\text{GEOM_MEAN} &= \left( \prod_i t_i^{w_i} \right)^{1/\sum w_i}
$$

In the example below, we consider three different targets (all associated with a
different goal) and give twice as much importance to the first target relative to each
of the other two:

```python
from baybe.targets import NumericalTarget
from baybe.objectives import DesirabilityObjective

target_1 = NumericalTarget(name=""t_1"", mode=""MIN"", bounds=(0, 100))
target_2 = NumericalTarget(name=""t_2"", mode=""MIN"", bounds=(0, 100))
target_3 = NumericalTarget(name=""t_3"", mode=""MATCH"", bounds=(40, 60))
objective = DesirabilityObjective(
    targets=[target_1, target_2, target_3],
    weights=[2.0, 1.0, 1.0],  # optional (by default, all weights are equal)
    scalarizer=""GEOM_MEAN"",  # optional
)
```

For a complete example demonstrating desirability mode, see [here]().",What scalarization functions are available in BayBE's DesirabilityObjective for combining multiple targets?,How do I combine multiple targets?
70,"# Objective
## ParetoObjective

The [`ParetoObjective`]() can be used when the goal is to find a set of solutions that represent optimal trade-offs among multiple conflicting targets. Unlike the [`DesirabilityObjective`](), this approach does not aggregate the targets into a single scalar value but instead seeks to identify the Pareto front - the set of *non-dominated* target configurations.

Tip: A target configuration is considered non-dominated (or Pareto-optimal) if no other configuration is better in *all* targets.

Identifying the Pareto front requires maintaining explicit models for each of the targets involved. Accordingly, it requires to use acquisition functions capable of processing vector-valued input, such as `qLogNoisyExpectedHypervolumeImprovement`. This differs from the [`DesirabilityObjective`](), which relies on a single predictive model to describe the associated desirability values. However, the drawback of the latter is that the exact trade-off between the targets must be specified *in advance*, through explicit target weights. By contrast, the Pareto approach allows to specify this trade-off *after* the experiments have been carried out, giving the user the flexibly to adjust their preferences post-hoc - knowing that each of the obtained
points is optimal with respect to a particular preference model.

To set up a [`ParetoObjective`](), simply specify the corresponding target objects:

```python
from baybe.targets import NumericalTarget
from baybe.objectives import ParetoObjective

target_1 = NumericalTarget(name=""t_1"", mode=""MIN"")
target_2 = NumericalTarget(name=""t_2"", mode=""MAX"")
objective = ParetoObjective(targets=[target_1, target_2])
```",What is the purpose of the ParetoObjective in BayBE?,What is the difference between a Pareto objective and a desirability objective?
71,"# Parameters

Parameters are fundamental for BayBE, as they configure the [`SearchSpace`]() and serve
as the direct link to the controllable variables in your experiment.
Before starting an iterative campaign, the user is required to specify the exact
parameters they can control and want to consider in their optimization.
note
BayBE identifies each parameter by a ``name``. All parameter names in one 
campaign must be unique.
BayBE distinguishes two parameter types, because they need to be treated very
differently under the hood: Discrete and continuous parameters.

## Continuous Parameters
### NumericalContinuousParameter

This is currently the only continuous parameter type BayBE supports.
It defines possible values from a numerical interval called
`bounds`, and thus has an infinite amount of possibilities.
Unless restrained by [`Constraint`]()s, BayBE will consider any possible parameter value
that lies within the chosen interval.

```python
from baybe.parameters import NumericalContinuousParameter

NumericalContinuousParameter(
    name=""Temperature"",
    bounds=(0, 100),
)
```",Can I have two parameters with the same name in one campaign?,What is the only continuous parameter type currently supported by BayBE?
72,"# Parameters
## Discrete Parameters
A discrete parameter has a finite set of possible values that can be recommended.
These values can be numeric or label-like (i.e. strings) and are transformed
internally before being ingested by the surrogate model.

### NumericalDiscreteParameter

This is the right type for parameters that have numerical values.
We support sets with equidistant values like `(1, 2, 3, 4, 5)` but also unevenly
spaced sets of numbers like `(0.2, 1.0, 2.0, 5.0, 10.0, 50.0)`.

```python
from baybe.parameters import NumericalDiscreteParameter

NumericalDiscreteParameter(
    name=""Temperature"",
    # you can also use np.arange or similar to provide values
    values=(0, 10, 20, 30, 40, 50),
)
```",Which parameter type do I use if I allow my variables to have discrete and numerical values?,"What type should you use in BayBE for a parameter that can take a discrete set of numerical values like (0, 10, 20, 30, 40, 50)?"
73,"# Parameters
## Discrete Parameters
### CategoricalParameter

A [`CategoricalParameter`]() supports sets of strings as labels.
This is most suitable if the experimental choices cannot easily be translated into a
number.
Examples for this could be vendors like `(""Vendor A"", ""Vendor B"", ""Vendor C"")` or
post codes like `(""PO16 7GZ"", ""GU16 7HF"", ""L1 8JQ"")`.

Categorical parameters in BayBE can be encoded via integer or one-hot encoding.
For some cases, such basic forms of encoding make sense, e.g. if we had a parameter
for a setting with values
`(""low"", ""medium"", ""high"")`, an integer-encoding into values `(1, 2, 3)` would
be reasonable.

```python
from baybe.parameters import CategoricalParameter

CategoricalParameter(
    name=""Intensity"",
    values=(""low"", ""medium"", ""high""),
    active_values=(
        ""low"",  # optional, only combinations with Intensity=low will be recommended
    ),
    encoding=""INT"",  # optional, uses integer encoding as described above
)
```

However, in other cases, these encodings would introduce undesired biases to the model.
Take, for instance, a parameter for a choice of solvents with values
`(""Solvent A"", ""Solvent B"", ""Solvent C"")`. Encoding these with `(1, 2, 3)` as
above would imply that âSolvent Aâ is more similar to âSolvent Bâ than to âSolvent Câ,
simply because the number 1 is closer to 2 than to 3.
Hence, for an arbitrary set of labels, such an ordering cannot generally be assumed.
In the particular case of substances, it not even possible to describe the similarity
between labels by ordering along one single dimension.
For this reason, we also provide the [`SubstanceParameter`](), which encodes labels
corresponding to small molecules with chemical descriptors, capturing their similarities
much better and without the need for the user to think about ordering and similarity
in the first place.
This concept is generalized in the [`CustomDiscreteParameter`](), where the user can
provide their own custom set of descriptors for each label.",I want to model different bases for my reaction,What types of encoding are supported for CategoricalParameter in BayBE?
74,"# Parameters
## Discrete Parameters
### SubstanceParameter

Instead of `values`, this parameter accepts `data` in form of a dictionary. The
items correspond to pairs of labels and [SMILES](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system).
SMILES are string-based representations of molecular structures.
Based on these, BayBE can assign each label a set of molecular descriptors as encoding.

For instance, a parameter corresponding to a choice of solvents can be initialized with:

```python
from baybe.parameters import SubstanceParameter

SubstanceParameter(
    name=""Solvent"",
    data={
        ""Water"": ""O"",
        ""1-Octanol"": ""CCCCCCCCO"",
        ""Toluene"": ""CC1=CC=CC=C1"",
    },
    active_values=[  # optional, recommends only water and toluene as solvent
        ""Water"",
        ""Toluene"",
    ],
    encoding=""MORDRED"",  # optional
    decorrelate=0.7,  # optional
)
```

The `encoding` defines what kind of descriptors are calculated using the
[scikit-fingerprints](https://scikit-fingerprints.github.io/scikit-fingerprints/) package.
It can be specified either by passing the corresponding [`SubstanceEncoding`]() member
(click to see full list of options) or its string representation, e.g. use
[`SubstanceParameter.MORDRED`]()
or its string alias `""MORDRED""` to select the `MordredFingerprint`.

Here are examples of a few popular fingerprints:

* `ECFP`: Extended Connectivity FingerPrint,
  which is a circular topological fingerprint similar to Morgan fingerprint.
* `MORDRED`: Chemical descriptor based fingerprint.
* `RDKIT`: The RDKit fingerprint, which is based on hashing of molecular subgraphs.

You can customize the fingerprint computation by passing arguments of the corresponding
[scikit-fingerprints](https://scikit-fingerprints.github.io/scikit-fingerprints/) class to the `kwargs_fingerprint` argument the [`SubstanceParameter`]() constructor.
Similarly, for fingerprints requiring conformers,
the configuration options for conformer computation can be specified via `kwargs_conformer`.

```python
from baybe.parameters import SubstanceParameter

SubstanceParameter(
    name=""Solvent"",
    data={
        ""Water"": ""O"",
        ""1-Octanol"": ""CCCCCCCCO"",
        ""Toluene"": ""CC1=CC=CC=C1"",
    },
    encoding=""ECFP"",
    kwargs_fingerprint={
        ""radius"": 4,  # Set maximum radius of resulting subgraphs
        ""fp_size"": 1024,  # Change the number of computed bits
    },
)

```

These calculations will typically result in 500 to 1500 numbers per molecule.
To avoid detrimental effects on the surrogate model fit, we reduce the number of
descriptors via decorrelation before using them.
For instance, the `decorrelate` option in the example above specifies that only
descriptors with a correlation lower than 0.7 to any other descriptor will be kept.
This usually reduces the number of descriptors to 10-50, depending on the specific
items in `data`.

#### WARNING
The descriptors calculated for a [`SubstanceParameter`]() were developed to describe
small molecules and are not suitable for other substances. If you deal with large
molecules like polymers or arbitrary substance mixtures, we recommend to provide your
own descriptors via the [`CustomDiscreteParameter`]().

:class: note
The ``SubstanceParameter`` is only available if BayBE was installed with the 
additional ``chem`` dependency.",How do I create chemical SubstanceParameter?,"What does the encoding option ""MORDRED"" specify when initializing a SubstanceParameter in BayBE?"
75,"# Parameters
## Discrete Parameters
### CustomDiscreteParameter

The `encoding` concept introduced above is generalized by the
[`CustomDiscreteParameter`]().
Here, the user is expected to provide their own descriptors for the encoding.

Take, for instance, a parameter that corresponds to the choice of a polymer.
Polymers are not well represented by the small molecule descriptors utilized in the
[`SubstanceParameter`]().
Still, one could provide experimental measurements or common metrics used to classify
polymers:

```python
import pandas as pd
from baybe.parameters import CustomDiscreteParameter

descriptors = pd.DataFrame(
    {
        ""Glass_Transition_TempC"": [20, -71, -39],
        ""Weight_kDalton"": [120, 32, 241],
    },
    index=[""Polymer A"", ""Polymer B"", ""Polymer C""],  # put labels in the index
)

CustomDiscreteParameter(
    name=""Polymer"",
    data=descriptors,
    active_values=(  # optional, enforces that only Polymer A or C is recommended
        ""Polymer A"",
        ""Polymer C"",
    ),
    decorrelate=True,  # optional, uses default correlation threshold
)
```

With the [`CustomDiscreteParameter`](), you can also encode parameter labels that have
nothing to do with substances.
For example, a parameter corresponding to the choice of a vendor is typically not
easily encoded with standard means.
In BayBEâs framework, you can provide numbers corresponding e.g. to delivery time,
reliability or average price of the vendor to encode the labels via the
[`CustomDiscreteParameter`]().","How can you encode a parameter in BayBE that represents a choice like vendor, using custom metrics such as delivery time or reliability?","I am optimizing an experiment where one of the parameters is the choice of catalyst. I have three specific catalysts, and I know their key performance characteristics like 'surface area', 'particle size', and 'cost'. These are not standard chemical descriptors. How can I provide this custom data to BayBE so that the recommendation model understands the underlying properties of my catalysts instead of just treating them as three unrelated options?"
76,"# Parameters
## Discrete Parameters
### TaskParameter

Often, several experimental campaigns involve similar or even identical parameters but
still have one or more differences.
For example, when optimizing reagents in a chemical reaction, the reactants remain
constant, so they are not parameters.
Similarly, in a mixture development for cell culture media, the cell type is fixed and
hence not a parameter.
However, once we plan to mix data from several campaigns, both reactants and cell
lines can also be considered parameters in that they encode the necessary context.
BayBE is able to process such context information with the [`TaskParameter`]().
In many cases, this can drastically increase the optimization performance due to the
enlarged data corpus.

#### SEE ALSO
For details, refer to [transfer learning](transfer_learning.md).",What type of parameter does BayBE use to encode context information when mixing data from several experimental campaigns?,What do I do if I have parameters which were constant but are now subject to change when I add more data?
77,"# Recommenders

## General Information

Recommenders are an essential part of BayBE that effectively explore the search space
and provide recommendations for the next experiment or batch of experiments.
Available recommenders can be partitioned into the following subclasses.

## Pure Recommenders

Pure recommenders simply take on the task to recommend measurements. They each contain
the inner logic to do so via different algorithms and approaches.
While some pure recommenders are versatile and work across different types of search
spaces, other are specifically designed for discrete or continuous spaces. The
compatibility is indicated via the corresponding `compatibility` class variable.",What is a recommender?,What is the purpose of pure recommenders in BayBE?
78,"# Recommenders
## Pure Recommenders
### Bayesian Recommenders

The Bayesian recommenders in BayBE are built on the foundation of the
[`BayesianRecommender`]()
class, offering an array of possibilities with internal surrogate models and support
for various acquisition functions.

* The **[`BotorchRecommender`]()**
  is a powerful recommender based on BoTorchâs optimization engine that can be applied
  to all kinds of search spaces. In continuous spaces, its `sequential_continuous` flag
  allows to choose between greedy sequential optimization and batch optimization as the
  underlying point generation mode. In discrete/hybrid spaces, sequential greedy
  selection is the only available mode and is thus activated automatically.

  Note that the recommender performs a brute-force search when applied to hybrid search
  spaces, as it does gradient-based optimization in the continuous part of the space
  while exhaustively evaluating configurations of the discrete subspace. You can customize this
  behavior to only sample a certain percentage of the discrete subspace via the
  `sampling_percentage`
  argument and to choose different sampling algorithms via the
  `hybrid_sampler`
  argument.

  The gradient-based optimization part can also further be controlled by the
  `n_restarts` and
  `n_raw_samples`
  arguments. For details, please refer
  to [BotorchRecommender]().
* The **[`NaiveHybridSpaceRecommender`]()**
  can be applied to all search spaces, but is intended to be used in hybrid spaces.
  This recommender combines individual recommenders for the continuous and the discrete
  subspaces. It independently optimizes each subspace and consolidates the best results
  to generate a candidate for the original hybrid space.",My optimization is too slow. How can I speed up?,What argument allows you to control the percentage of the discrete subspace sampled by the BotorchRecommender in hybrid search spaces?
79,"# Recommenders
## Pure Recommenders
### Clustering Recommenders

BayBE offers a set of recommenders leveraging techniques to facilitate point selection
via clustering:

* **[`PAMClusteringRecommender`]():**
  This recommender utilizes partitioning around medoids.
* **[`KMeansClusteringRecommender`]():**
  This recommender implements k-means clustering.
* **[`GaussianMixtureClusteringRecommender`]():**
  This recommender leverages Gaussian Mixture Models for clustering.",Which clustering recommender in BayBE uses Gaussian Mixture Models?,Which clustering recommender does BayBE have?
80,"# Recommenders
## Pure Recommenders
### Sampling Recommenders

BayBE provides two recommenders that recommend by sampling form the search space:

* **[`RandomRecommender`]():**
  This recommender offers random recommendations for all types of search spaces.
  It is extensively used in backtesting examples, providing a valuable comparison.

* **[`FPSRecommender`]():**
  This recommender is only applicable for discrete search spaces, and recommends points
  based on farthest point sampling.",How does BayBE get initial recommendations?,Which BayBE recommender is only applicable for discrete search spaces and recommends points using farthest point sampling?
81,"# Recommenders
## Meta Recommenders

In analogy to meta studies, meta recommenders are wrappers that operate on a sequence
of pure recommenders and determine when to switch between them according to different
logics. BayBE offers three distinct kinds of meta recommenders.

* The
  [`TwoPhaseMetaRecommender`]()
  employs two distinct recommenders and switches between them at a certain specified
  point, controlled by the `switch_after` attribute. This is useful e.g. if you want a
  different recommender for the initial recommendation when there is no data yet
  available. This simple example would recommend randomly for the first batch and switch
  to a Bayesian recommender as soon as measurements have been ingested:

```python
from baybe.recommenders import (
    BotorchRecommender,
    TwoPhaseMetaRecommender,
    RandomRecommender,
)

recommender = TwoPhaseMetaRecommender(
    initial_recommender=RandomRecommender(), recommender=BotorchRecommender()
)
```

* The **[`SequentialMetaRecommender`]()**
  introduces a simple yet versatile approach by utilizing a predefined list of
  recommenders. By specifying the desired behavior using the `mode` attribute, it is
  possible to flexibly determine the meta recommenderâs response when it exhausts the
  available recommenders. The possible choices are to either raise an error, re-use the
  last recommender or re-start at the beginning of the sequence.
* Similar to the `SequentialMetaRecommender`, the
  **[`StreamingSequentialMetaRecommender`]()**
  enables the utilization of *arbitrary* iterables to select recommender.

  #### WARNING
  Due to the arbitrary nature of iterables that can be used, (de-)serializability cannot
  be guaranteed. As a consequence, using a `StreamingSequentialMetaRecommender` results
  in an error if you attempt to serialize the corresponding object or higher-level
  objects containing it.","I have a multi-stage experimental strategy. I want to start with a few random batches to get initial data, then switch to a different recommender for exploration, and finally use the powerful ?BotorchRecommender for exploitation. The ?TwoPhaseMetaRecommender seems too simple for this. How can I implement this three-stage sequence, and what happens once the sequence is finished?",What attribute controls when the TwoPhaseMetaRecommender switches between recommenders in BayBE?
82,"# Search Spaces

The term âsearch spaceâ refers to the domain of possible values for the parameters that are being optimized during a campaign. A search space represents the space within which BayBE explores and searches for the optimal solution. It is implemented via the [`SearchSpace`]() class.

Note that a search space is not necessarily equal to the space of allowed measurements. That is, if configured properly, it is possible to add measurements to a campaign that are not part of the search space. For instance, a numerical parameter with values `1.0`, `2.0`, `5.0` will create a searchspace with these numbers, but you can also add measurements where the parameter has a value of e.g. `2.12`.

In BayBE, a search space is a union of two (potentially empty) subspaces. The [`SubspaceDiscrete`]() contains all discrete parameters, while the [`SubspaceContinuous`]() contains all continuous parameters.

Depending on which of the subspaces are non-empty, a `SearchSpace` has exactly one of the three [`SearchSpaceType`]()âs:

| `SubspaceDiscrete`   | `SubspaceContinuous`   | [`SearchSpaceType`]()            |
|----------------------|------------------------|----------------------------------|
| Non-empty            | Empty                  | [`SearchSpaceType.DISCRETE`]()   |
| Empty                | Non-Empty              | [`SearchSpaceType.CONTINUOUS`]() |
| Non-Empty            | Non-empty              | [`SearchSpaceType.HYBRID`]()     |",What are the three possible types of SearchSpaceType in BayBE?,"Can you explain search space, subspace and search space type?"
83,"# Search Spaces
## Constructing Full Search Spaces

There are several methods available for creating full search spaces.
### From the Default Constructor

It is possible to construct a search space by simply using the default constructor of the `SearchSpace` class.
The required parameters are derived from the `__init__` function of that class.
In the simplest setting, it is sufficient to provide a single subspace for creating either a discrete or continuous search, or provide two subspaces for creating a hybrid search space.

```python
searchspace = SearchSpace(discrete=discrete_subspace, continuous=continuous_subspace)
```

While this constructor is the default choice, it might not be the most convenient.
Consequently, other constructors are available.### Building from the Product of Parameter Values

The function [`SearchSpace.from_product`]() is analog to the corresponding function available for `SubspaceDiscrete`, but allows the parameter list to contain both discrete and continuous parameters.",How can I create full search space?,How can you create a hybrid search space in BayBE using the SearchSpace class?
84,"# Search Spaces
## Constructing Full Search Spaces
### Constructing from a Dataframe

[`SearchSpace.from_dataframe`]() constructs a search space from a given dataframe.
Due to the ambiguity between discrete and continuous parameter representations when identifying parameter ranges based only on data, this function requires that the appropriate parameter definitions be explicitly provided. This is different for its subspace counterparts [`SubspaceDiscrete.from_dataframe`]() and [`SubspaceContinuous.from_dataframe`](), where a fallback mechanism can automatically infer minimal parameter specifications if omitted.

```python
from baybe.searchspace import SearchSpace

p_cont = NumericalContinuousParameter(name=""c"", bounds=[0, 1])
p_disc = NumericalDiscreteParameter(name=""d"", values=[1, 2, 3])
df = pd.DataFrame({""c"": [0.3, 0.7], ""d"": [2, 3]})
searchspace = SearchSpace.from_dataframe(df=df, parameters=[p_cont, p_disc])
print(searchspace)
```

```default
SearchSpace
   Search Space Type: HYBRID
   SubspaceDiscrete
      Discrete Parameters
           Name                        Type  Num_Values Encoding
         0    d  NumericalDiscreteParameter           3     None
      Experimental Representation
            d
         0  2
         1  3
      Constraints
         Empty DataFrame
         Columns: []
         Index: []
      Computational Representation
            d
         0  2
         1  3
   SubspaceContinuous
      Continuous Parameters
           Name                          Type  Lower_Bound  Upper_Bound
         0    c  NumericalContinuousParameter          0.0          1.0
      Linear Equality Constraints
         Empty DataFrame
         Columns: []
         Index: []
      Linear Inequality Constraints
         Empty DataFrame
         Columns: []
         Index: []
      Non-linear Constraints
         Empty DataFrame
         Columns: []
         Index: []
```",What must be explicitly provided when using SearchSpace.from_dataframe to construct a search space from a dataframe?,Can I create a search space from a dataframe with continuous and discrete data?
85,"# Search Spaces
## Restricting Search Spaces Using Constraints

Most constructors for both subspaces and search spaces support the optional keyword argument `constraints` to provide a list of [`Constraint`]() objects.
When constructing full search spaces, the type of each constraint is checked, and the consequently applied to the corresponding subspace.

```python
constraints = [...]
# Using one example constructor here
searchspace = SearchSpace.from_product(parameters=parameters, constraints=constraints)
```

",How can you restrict search spaces in BayBE?,Can I define constraints for my parameters?
86,"# Search Spaces
## Discrete Subspaces
The `SubspaceDiscrete` contains all the discrete parameters of a `SearchSpace`. There are different ways of constructing this subspace.

### Building from the Product of Parameter Values

The method [`SearchSpace.from_product`]() constructs the full cartesian product of the provided parameters:

```python
from baybe.parameters import NumericalDiscreteParameter, CategoricalParameter
from baybe.searchspace import SubspaceDiscrete

parameters = [
    NumericalDiscreteParameter(name=""x0"", values=[1, 2, 3]),
    NumericalDiscreteParameter(name=""x1"", values=[4, 5, 6]),
    CategoricalParameter(name=""Speed"", values=[""slow"", ""normal"", ""fast""]),
]
subspace = SubspaceDiscrete.from_product(parameters=parameters)
```

In this example, `subspace` has a total of 27 different parameter configuration.

```default
      x0   x1   Speed
 0   1.0  4.0    slow
 1   1.0  4.0  normal
 2   1.0  4.0    fast
 ..  ...  ...     ...
 24  3.0  6.0    slow
 25  3.0  6.0  normal
 26  3.0  6.0    fast

  [27 rows x 3 columns]
```",Is it possible to fully enumerate a search space combinatorially?,How can you construct a discrete subspace that contains all possible combinations of given parameter values in BayBE?
87,"# Search Spaces
## Discrete Subspaces
### Constructing from a Dataframe

[`SubspaceDiscrete.from_dataframe`]() constructs a discrete subspace from a given dataframe.
By default, this method tries to infer the data column as as a [`NumericalDiscreteParameter`]() and uses [`CategoricalParameter`]() as fallback.
However, it is possible to change this behavior by using the optional `parameters` keyword.
This list informs `from_dataframe` about the parameters and the types of parameters that should be used.
In particular, it is necessary to provide such a list if there are non-numerical parameters that should not be interpreted as categorical parameters.

```python
import pandas as pd

df = pd.DataFrame(
    {
        ""x0"": [2, 3, 3],
        ""x1"": [5, 4, 6],
        ""x2"": [9, 7, 9],
    }
)
subspace = SubspaceDiscrete.from_dataframe(df)
```

```default
 Discrete Parameters
   Name                        Type  Num_Values Encoding
 0   x0  NumericalDiscreteParameter           2     None
 1   x1  NumericalDiscreteParameter           3     None
 2   x2  NumericalDiscreteParameter           2     None
```",What is the default parameter type inferred by SubspaceDiscrete.from_dataframe for a data column in BayBE?,How do I convert my pandas dataframe into a BayBE search space?
88,"# Search Spaces
## Discrete Subspaces
### Creating a Simplex-Bound Discrete Subspace

[`SubspaceDiscrete.from_simplex`]() can be used to efficiently create a discrete search space (or discrete subspace) that is restricted by a simplex constraint, limiting the maximum sum of the parameters per dimension.
This method uses a shortcut that removes invalid candidates already during the creation of parameter combinations and avoids to first create the full product space before filtering it.

In the following example, a naive construction of the subspace would first construct the full product space, containing 25 points, although only 15 points are actually part of the simplex.

```python
parameters = [
    NumericalDiscreteParameter(name=""p1"", values=[0, 0.25, 0.5, 0.75, 1]),
    NumericalDiscreteParameter(name=""p2"", values=[0, 0.25, 0.5, 0.75, 1]),
]
subspace = SubspaceDiscrete.from_simplex(max_sum=1.0, simplex_parameters=parameters)
```

```default
       p1    p2
 0   0.00  0.00
 1   0.00  0.25
 2   0.00  0.50
 ..   ...   ...
 12  0.75  0.00
 13  0.75  0.25
 14  1.00  0.00

 [15 rows x 2 columns]
```

Note that it is also possible to provide additional parameters that then enter in the form of a Cartesian product.
These can be provided via the keyword `product_parameters`.
",Which method can be used to create a discrete subspace with a simplex constraint in BayBE?,How can I create a discrete search space?
89,"# Search Spaces
## Discrete Subspaces
### Representation of Data within Discrete Subspaces

Internally, discrete subspaces are represented by two dataframes, the *experimental* and the *computational* representation.

The experimental representation (`exp_rep`) contains all parameters as they were provided upon the construction of the search space and viewed by the experimenter. The computational representation (`comp_rep`) contains a representation of parameters that is actually used for the internal calculation.

In particular, the computational representation contains no more labels or constant columns. This happens e.g. for [`SubstanceParameter`]() or [`CategoricalParameter`](). Further, note that the shape of the computational representation can also change depending on the chosen encoding.

The following example demonstrates the difference:

```python
from baybe.parameters import NumericalDiscreteParameter, CategoricalParameter

speed = CategoricalParameter(""Speed"", values=[""slow"", ""normal"", ""fast""], encoding=""OHE"")
temperature = NumericalDiscreteParameter(name=""Temperature"", values=[90, 105])

subspace = SubspaceDiscrete.from_product(parameters=[speed, temperature])
```

```default
  Experimental Representation
      Speed  Temperature
  0    slow         90.0
  1    slow        105.0
  2  normal         90.0
  3  normal        105.0
  4    fast         90.0
  5    fast        105.0

  Computational Representation
     Speed_slow  Speed_normal  Speed_fast  Temperature
  0           1             0           0         90.0
  1           1             0           0        105.0
  2           0             1           0         90.0
  3           0             1           0        105.0
  4           0             0           1         90.0
  5           0             0           1        105.0
```",What are the names of the two internal dataframes used to represent discrete subspaces in BayBE?,How is the data contained in a search space being represented and handled internally?
90,"# Search Spaces
## Continuous Subspaces

The `SubspaceContinuous` contains all the continuous parameters of a `SearchSpace`. There are different ways of constructing this subspace.

### Using Explicit Bounds

The [`SubspaceContinuous.from_bounds`]() method can be used to easily create a subspace representing a hyperrectangle.

```python
from baybe.searchspace import SubspaceContinuous

bounds = pd.DataFrame({""param1"": [0, 1], ""param2"": [-1, 1]})
subspace = continuous = SubspaceContinuous.from_bounds(bounds)
```

```default
 Continuous Parameters
      Name                          Type  Lower_Bound  Upper_Bound
 0  param1  NumericalContinuousParameter          0.0          1.0
 1  param2  NumericalContinuousParameter         -1.0          1.0
```",How can I create a subspace representing a hyperrectangle?,How can you create a continuous subspace with explicit bounds in BayBE?
91,"# Search Spaces
## Continuous Subspaces
### Constructing from a Dataframe

Similar to discrete subspaces, continuous spaces can also be constructed using [`SubspaceContinuous.from_dataframe`]().
However, when using this method to create a continuous space, it will create the smallest axis-aligned hyperrectangle-shaped continuous subspace that contains the points specified in the given dataframe.

```python
from baybe.parameters import NumericalContinuousParameter
from baybe.searchspace.continuous import SubspaceContinuous

points = pd.DataFrame(
    {
        ""param1"": [0, 1, 2],
        ""param2"": [-1, 0, 1],
    }
)
subspace = SubspaceContinuous.from_dataframe(df=points)
```

As for discrete subspaces, this method automatically infers the parameter types but can be provided with an optional list `parameters`.

```default
 Continuous Parameters
      Name                          Type  Lower_Bound  Upper_Bound
 0  param1  NumericalContinuousParameter          0.0          2.0
 1  param2  NumericalContinuousParameter         -1.0          1.0
```",What shape of continuous subspace does SubspaceContinuous.from_dataframe create from the points in a dataframe?,How do I create a search space from continuous data?
92,"# Serialization
## Deserialization from configuration strings
### Invoking alternative constructors

Many BayBE classes offer additional routes of construction next to the default
mechanism via the classâ `__init__` method.
This offers convenient ways of object initialization alternative to specifying
an objectâs attributes in their âcanonicalâ form, which is often not the preferred
approach.

For instance, a search space is composed of two sub-components, a
[discrete subspace]()
and a [continuous subspace](),
which are accordingly expected by the
[`SearchSpace`]() constructor.
However, instead of providing the two components directly, most users would more
naturally invoke one of the alternative class methods available, such as
`SearchSpace.from_product` or
`SearchSpace.from_dataframe`.

Using a serialization string, the same alternative routes can be triggered via the
optional `constructor` field that allows specifying the initializer to be used for the
object creation step:

```python
from baybe.searchspace import SearchSpace
from baybe.parameters import CategoricalParameter, NumericalDiscreteParameter

searchspace = SearchSpace.from_product(
    parameters=[
        CategoricalParameter(name=""Category"", values=[""low"", ""high""]),
        NumericalDiscreteParameter(name=""Number"", values=[1, 2, 3]),
    ]
)

searchspace_json = """"""
{
    ""constructor"": ""from_product"",
    ""parameters"": [
        {
            ""type"": ""CategoricalParameter"",
            ""name"": ""Category"",
            ""values"": [""low"", ""high""]
        },
        {
            ""type"": ""NumericalDiscreteParameter"",
            ""name"": ""Number"",
            ""values"": [1, 2, 3]
        }
    ]
}
""""""

assert searchspace == SearchSpace.from_json(searchspace_json)
```",How can you specify which alternative constructor to use when deserializing a BayBE object from a configuration string?,How can I store my results?
93,"# Serialization
## Deserialization from configuration strings
### Dataframe deserialization

When serializing BayBE objects, contained [`DataFrames`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas.DataFrame) are
automatically converted to a binary format in order to

1. ensure that the involved data types are exactly restored after completing the roundtrip and
2. decrease the size of the serialization string through compression.

From the userâs perspective, this has the disadvantage that the resulting JSON
representation is not human-readable, which can be a challenge when working
with configuration strings.

While you can manually work around this additional conversion step using our
`serialize_dataframe` and
`deserialize_dataframe` helpers,
a more elegant solution becomes apparent when noticing that [invoking alternative
constructors](#alternative-constructors) also works for non-BayBE objects.
In particular, this means you can resort to any dataframe constructor of your choice
(such as [`DataFrame.from_records`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.from_records.html#pandas.DataFrame.from_records))
when defining your configuration, instead of having to work with compressed formats:

```python
import pandas as pd
from baybe.searchspace.discrete import SubspaceDiscrete

subspace = SubspaceDiscrete.from_dataframe(
    pd.DataFrame.from_records(
        data=[[1, ""a""], [2, ""b""], [3, ""c""]], columns=[""Number"", ""Category""]
    )
)

subspace_json = """"""
{
    ""constructor"": ""from_dataframe"",
    ""df"": {
        ""constructor"": ""from_records"",
        ""data"": [[1, ""a""], [2, ""b""], [3, ""c""]],
        ""columns"": [""Number"", ""Category""]
    }
}
""""""
reconstructed = SubspaceDiscrete.from_json(subspace_json)

assert subspace == reconstructed
```",How does BayBE ensure that DataFrames are exactly restored after serialization and deserialization?,My DataFrame is serialized to JSON that I don't understand. How can I read it and edit it?
94,"# Serialization

BayBE is shipped with a sophisticated serialization engine that allows to unstructure
its objects into basic types and seamlessly reassemble them afterward.
This enables a variety of advanced workflows, such as:

* Persisting objects for later use
* Transmission and processing outside the Python ecosystem
* Interaction with APIs and databases
* Writing âconfigurationâ files

Some of these workflows are demonstrated in the sections below.

## JSON (de-)serialization

Most BayBE objects can be conveniently serialized into an equivalent JSON
representation by calling their
`to_json` method.
The obtained JSON string can then be deserialized via the
`from_json` method
of the corresponding class, which yields an âequivalent copyâ of the original object.

For example:

```python
from baybe.parameters import CategoricalParameter

parameter = CategoricalParameter(name=""Setting"", values=[""low"", ""high""])
json_string = parameter.to_json()
reconstructed = CategoricalParameter.from_json(json_string)
assert parameter == reconstructed
```

This form of roundtrip serialization can be used, for instance, to persist objects
for long-term storage, but it also provides an easy way to âmoveâ existing objects
between Python sessions by executing the deserializing step in a different context
than the serialization step.",Does BayBE offer serialization?,How can BayBE objects be serialized and deserialized to and from JSON?
95,"# Serialization
## Deserialization from configuration strings

The workflow described [above](#json-serialization) most naturally applies to
situations where we start inside the Python ecosystem and want to make an object
leave the running session.
However, in many cases, we would like to kickstart the process from the other end and
rather specify a BayBE object **outside** Python for use in a later computation.
Common examples are when we wish to interact with an API or simply want to persist
a certain BayBE component in the form of a âconfigurationâ file.

The following sections give an overview of the flexibilities that are offered for this
task. Of course, the underlying concepts can be mixed and matched arbitrarily.# Serialization
## Deserialization from configuration strings
### Basic string assembly

Writing a configuration for a certain BayBE object in form of a serialization string is
easy:

1. Select your desired object class
2. Identify the arguments expected by one of its constructors (see also [here](#alternative-constructors))
3. Pack them into a JSON string that mirrors the constructor signature

Letâs have a more detailed look, for instance, at the serialization string from
the [above example](#json-serialization), this time assuming we wanted to assemble
the string manually.
For this purpose, we have a peek at the signature of the `__init__` method of
`CategoricalParameter`
and notice that it has two required arguments, `name` and `values`.
We specify these accordingly as separate fields in the JSON string:

```python
from baybe.parameters import CategoricalParameter

parameter_json = """"""
{
    ""name"": ""Setting"",
    ""values"": [""low"", ""high""]
}
""""""
via_json = CategoricalParameter.from_json(parameter_json)
via_init = CategoricalParameter(name=""Setting"", values=[""low"", ""high""])

assert via_json == via_init
```",I want to use a parameter for an API call later. Is it possible to create a JSON string directly?,How do you deserialize a BayBE object from a configuration string?
96,"# Serialization
## Deserialization from configuration strings
### Using default values

Just like default values can be omitted when working in Python,
they can be omitted from the corresponding serialization string:

```python
from baybe.parameters import CategoricalParameter

p1 = CategoricalParameter(name=""Setting"", values=[""low"", ""high""])
p2 = CategoricalParameter(name=""Setting"", values=[""low"", ""high""], encoding=""OHE"")

p1_json = """"""
{
    ""name"": ""Setting"",
    ""values"": [""low"", ""high""]
}
""""""
p2_json = """"""
{
    ""name"": ""Setting"",
    ""values"": [""low"", ""high""],
    ""encoding"": ""OHE""
}
""""""

p1_via_json = CategoricalParameter.from_json(p1_json)
p2_via_json = CategoricalParameter.from_json(p2_json)

assert p1 == p1_via_json == p2 == p2_via_json
```",What happens if I omit a value from the serialization string?,How can default values be handled when deserializing a CategoricalParameter from a configuration string in BayBE?
98,"# Serialization
## Deserialization from configuration strings
### The type field

Due to the leading design philosophy behind BayBE to provide its users easy access
to a broad range of tools, you typically have the choice between several modelling
alternatives when building your objects.
For example, when describing the degrees of freedom of your experimental campaign,
you can chose from several different [parameter types](parameters.md).

While this offers great flexibility, it comes with a challenge for deserialization
because you cannot know a priori which concrete object subclass is contained
in an incoming serialization string on the receiving end.
Instead, you oftentimes need to be able to process the incoming string dynamically.

For example, consider the following string, which perfectly mirrors the signatures of
both
`CategoricalParameter` and
`TaskParameter`:

```python
parameter_json = """"""
{
    ""name"": ""Setting"",
    ""values"": [""low"", ""high""]
}
""""""
```

Unless you are aware of the specific purpose for which the string was created,
calling one of the classesâ constructors directly is impossible because you
simply do not know which one to chose.
A similar situation arises with [nested objects](#nested-objects) because resorting to
an explicit constructor call of a hand-selected subclass is only possible at the
highest level of the hierarchy, whereas the inner object types would remain unspecified.

The problem can be easily circumvented using an explicit subclass resolution
mechanism, i.e., by tagging the respective subclass in an additional `type` field that
holds the classâ name.
This allows to deserialize the object from the corresponding base class instead
(i.e., `Parameter` class in the example below),
mirroring the flexibility of specifying subtypes to your configuration file:

```python
from baybe.parameters.base import Parameter
from baybe.parameters import CategoricalParameter, TaskParameter

categorical_parameter = CategoricalParameter(name=""Setting"", values=[""low"", ""high""])
categorical_parameter_json = """"""
{
    ""type"": ""CategoricalParameter"",
    ""name"": ""Setting"",
    ""values"": [""low"", ""high""]
}
""""""
# NOTE: we can use `Parameter.from_json` instead of `CategoricalParameter.from_json`:
categorical_parameter_reconstructed = Parameter.from_json(categorical_parameter_json)
assert categorical_parameter == categorical_parameter_reconstructed

task_parameter = TaskParameter(name=""Setting"", values=[""low"", ""high""])
task_parameter_json = """"""
{
    ""type"": ""TaskParameter"",
    ""name"": ""Setting"",
    ""values"": [""low"", ""high""]
}
""""""
# NOTE: we can use `Parameter.from_json` instead of `TaskParameter.from_json`:
task_parameter_reconstructed = Parameter.from_json(task_parameter_json)
assert task_parameter == task_parameter_reconstructed
```

#### NOTE
When serializing an object that belongs to a class hierarchy, BayBE automatically
injects the `type` field into the serialization string to enable frictionless deserialization
at a later stage.","What is the purpose of the ""type"" field in BayBE's serialization strings?","When deserializing nested objects with different object types, how do you make sure they are deserialized to the correct object subclasses?"
99,"# Serialization
## Deserialization from configuration strings
### Using abbreviations

Classes that have an `abbreviation` class variable defined can be conveniently
deserialized using the corresponding abbreviation string:

```python
from baybe.acquisition.base import AcquisitionFunction

acqf1 = AcquisitionFunction.from_json('{""type"": ""UpperConfidenceBound""}')
acqf2 = AcquisitionFunction.from_json('{""type"": ""UCB""}')

assert acqf1 == acqf2
```

<a id=""nested-objects""></a>",How can classes with an abbreviation class variable be deserialized in BayBE?,How to deserialize a class with abbreviated class variables?
100,"# Serialization
## Deserialization from configuration strings
### Nesting objects

BayBE objects typically appear as part of a larger object hierarchy.
For instance, a
`SearchSpace` can hold one or several
`Parameters`, just like an
`Objective` can hold one or several
`Targets`.
This hierarchical structure can be directly replicated in the serialization string:

```python
from baybe.objectives import DesirabilityObjective
from baybe.targets import NumericalTarget

objective = DesirabilityObjective(
    targets=[
        NumericalTarget(name=""T1"", mode=""MAX"", bounds=(-1, 1)),
        NumericalTarget(name=""T2"", mode=""MIN"", bounds=(0, 1)),
    ],
    weights=[0.1, 0.9],
    scalarizer=""MEAN"",
)

objective_json = """"""
{
    ""targets"": [
        {
            ""type"": ""NumericalTarget"",
            ""name"": ""T1"",
            ""mode"": ""MAX"",
            ""bounds"": [-1.0, 1.0]
        },
        {
            ""type"": ""NumericalTarget"",
            ""name"": ""T2"",
            ""mode"": ""MIN"",
            ""bounds"": [0.0, 1.0]
        }
    ],
    ""weights"": [0.1, 0.9],
    ""scalarizer"": ""MEAN""
}
""""""

assert objective == DesirabilityObjective.from_json(objective_json)
```

<a id=""alternative-constructors""></a>",how do I format my input objective?,How do I encode targets in BayBE? How do I set different weights for targets? I have multiple targets
101,"# Simulation
BayBE offers multiple functionalities to âsimulateâ experimental campaigns with a given lookup mechanism. This user guide briefly introduces how to use the methods available in our [simulation subpackage]().

For a wide variety of applications of this functionality, we refer to the corresponding [examples]().

## Terminology: What do we mean by âSimulationâ?

The term âsimulationâ can have two slightly different interpretations, depending on the applied context.

1. It can refer to âbacktestingâ a particular experimental campaign on a fixed finite dataset.
   Thus, âsimulationâ means investigating what experimental trajectory we would have observed if we had used different setups or recommenders and restricted the possible parameter configurations to those contained in the dataset.
2. It can refer to the simulation of an *actual* DOE loop, i.e., recommending experiments and retrieving the corresponding measurements, where the loop closure is realized in the form of a callable (black-box) function that can be queried during the optimization to provide target values. Such a callable could for instance be a simple analytical function or a numerical solver of a set of differential equations that describe a physical system.","What are the two main interpretations of ""simulation"" in BayBE?",Is there a difference in how different types of simulation are handled by BayBE?
102,"# Simulation
## The Lookup Mechanism

BayBEâs simulation package enables a wide range of use cases and can even be used for âoracle predictionsâ.
This is made possible through the flexible use of lookup mechanisms, which act as the loop-closing element of an optimization loop.

Lookups can be provided in a variety of ways, by using fixed data sets, analytical functions, or any other form of black-box callable.
In all cases, their role is the same: to retrieve target values for parameter configurations suggested by the recommendation engine.# Simulation
## The Lookup Mechanism
### Using a `Callable`

Using a `Callable` is the most general way to provide :class: tip
If you already have a lookup callable available in an array-based format (for instance,
if your lookup values are generated using third-party code that works with array inputs
and outputs), you can effortlessly convert this callable into the required
dataframe-based format by applying our
{func}`~baybe.utils.dataframe.arrays_to_dataframes` decorator. 

For example, the above lookup can be equivalently created as follows:
```python
import numpy as np

from baybe.utils.dataframe import arrays_to_dataframes


@arrays_to_dataframes([""p1""], [""t1""])
def lookup(array: np.ndarray) -> np.ndarray:
    """"""The same lookup function in array logic.""""""
    return array**2
```a lookup mechanism.
Any `Callable` is a suitable lookup as long as it acce:class: tip
If you already have a lookup callable available in an array-based format (for instance,
if your lookup values are generated using third-party code that works with array inputs
and outputs), you can effortlessly convert this callable into the required
dataframe-based format by applying our
{func}`~baybe.utils.dataframe.arrays_to_dataframes` decorator. 

For example, the above lookup can be equivalently created as follows:
```python
import numpy as np

from baybe.utils.dataframe import arrays_to_dataframes


@arrays_to_dataframes([""p1""], [""t1""])
def lookup(array: np.ndarray) -> np.ndarray:
    """"""The same lookup function in array logic.""""""
    return array**2
```pts a dataframe containing parameter configurations and returns the corresponding target values.
More specifically:

- The input is expected to be a dataframe whose column names contain the parameter names and whose rows represent valid parameter configurations.
- The returned output must be a dataframe whose column names contain the target names and whose rows represent valid target values.
- The indices of the input and output dataframes must match.

An example might look like this:

```python
import pandas as pd

from baybe.parameters import NumericalContinuousParameter
from baybe.searchspace import SearchSpace
from baybe.targets import NumericalTarget

searchspace = SearchSpace.from_product(
    [
        NumericalContinuousParameter(""p1"", [0, 1]),
        NumericalContinuousParameter(""p2"", [-1, 1]),
    ]
)
objective = NumericalTarget(""t1"", ""MAX"").to_objective()


def lookup(df: pd.DataFrame) -> pd.DataFrame:
    """"""Map parameter configurations to target values.""""""
    return pd.DataFrame({""t1"": df[""p1""] ** 2}, index=df.index)


lookup(searchspace.continuous.sample_uniform(10))
```","If I have data in array format, can I use it as a lookup function in the simulation?",What decorator can be used in BayBE to convert an array-based lookup callable into the required dataframe-based format?
105,"# Simulation
## Simulating a Single Experiment

The function [`simulate_experiment`]() is the most basic form of simulation.
It runs a single execution of a DoE loop for either a specific number of iteration or until the search space is fully observed.

For using this function, it is necessary to provide a [`campaign`](). Although technically not necessary, we advise to also always provide a lookup mechanisms since fake results will be produced if none is provided. It is possible to specify several additional parameters like the batch size, initial data or the number of DoE iterations that should be performed

```python
results = simulate_experiment(
    # Necessary
    campaign=campaign,
    # Technically optional but should always be set
    lookup=lookup,
    # Optional
    batch_size=batch_size,
    n_doe_iterations=n_doe_iterations,
    initial_data=initial_data,
    random_seed=random_seed,
    impute_mode=impute_mode,
    noise_percent=noise_percent,
)
```

This function returns a dataframe that contains the results. For details on the columns of this dataframe as well as the dataframes returned by the other functions discussed here, we refer to the documentation of the subpackage [here]().",How to simulate a single experiment?,What does the simulate_experiment function in BayBE return?
106,"# Simulation
## Simulating Multiple Scenarios

The function [`simulate_scenarios`]() allows to specify multiple simulation settings at once.
Instead of a single campaign, this function expects a dictionary of campaigns, mapping scenario identifiers to `Campaign` objects.
In addition to the keyword arguments available for `simulate_experiment`, this function has two different keywords available:

1. `n_mc_iterations`: This can be used to perform multiple Monte Carlo runs with a single call. Multiple Monte Carlo runs are always advised to average out the effect of random effects such as the initial starting data.
2. `initial_data`: This can be used to provide a list of dataframe, where each dataframe is then used as initial data for an independent run. That is, the function performs one optimization loop per dataframe in this list.

Note that these two keywords are mutually exclusive.

```python
lookup = ...  # some reasonable lookup, e.g. a Callable
campaign1 = Campaign(...)
campaign2 = Campaign(...)
scenarios = {""Campaign 1"": campaign1, ""Campaign 2"": campaign2}

results = simulate_scenarios(
    scenarios=scenarios,
    lookup=lookup,
    batch_size=batch_size,
    n_doe_iterations=n_doe_iterations,
    n_mc_iterations=n_mc_iterations,
)
```","How do I simulate multiple scenarios? Im interested in running multiple campaigns, can you show me how to do that?",What are the two mutually exclusive keyword arguments available in the simulate_scenarios function in BayBE?
107,"# Simulation
## Simulating Transfer Learning

The function [`simulate_transfer_learning`]() partitions the search space into its tasks and simulates each task with the training data from the remaining tasks.

#### NOTE
Currently, this only supports discrete search spaces. See [`simulate_transfer_learning`]() for the reasons.

```python
task_param = TaskParameter(
    name=""Cell Line"",
    values=[""Liver Cell"", ""Brain Cell"", ""Skin Cell""],
)
# Define searchspace using a task parameter
searchspace = SearchSpace.from_product(parameters=[param1, param2, task_param])

# Create a suitable campaign
campaign = Campaign(searchspace=searchspace, objective=objective)

# Create a lookup dataframe. Note that this needs to have a column labeled ""Function""
# with values ""F1"" and ""F2""
lookup = DataFrame(...)

results = simulate_transfer_learning(
    campaign=campaign,
    lookup=lookup,
    batch_size=BATCH_SIZE,
    n_doe_iterations=N_DOE_ITERATIONS,
    n_mc_iterations=N_MC_ITERATIONS,
)
```",Can I use the `simulate_transfer_learning` function for continuous search spaces?,What type of search spaces does the simulate_transfer_learning function currently support in BayBE?
108,"# Surrogates

Surrogate models are used to model and estimate the unknown objective function of the
DoE campaign. BayBE offers a diverse array of surrogate models, while also allowing for
the utilization of custom models. All surrogate models are based upon the general
[`Surrogate`]() class. Some models even support transfer
learning, as indicated by the `supports_transfer_learning` attribute.

## Available Models

BayBE provides a comprehensive selection of surrogate models, empowering you to choose
the most suitable option for your specific needs. The following surrogate models are
available within BayBE:

* [`GaussianProcessSurrogate`]()
* [`BayesianLinearSurrogate`]()
* [`MeanPredictionSurrogate`]()
* [`NGBoostSurrogate`]()
* [`RandomForestSurrogate`]()

# Surrogates
## Multi-Output Modeling

Depending on the use case at hand, it may be necessary to model multiple output
variables simultaneously. However, not all surrogate types natively provide (joint)
predictive distributions for more than one variable, as indicated by their
`supports_multi_output` attribute.

In multi-output contexts, it may therefore be necessary to assemble several
single-output surrogates into a composite model to build a joint predictive model from
independent components for each output. BayBE provides two convenient mechanisms to
achieve this, both built upon the
`CompositeSurrogate` class:",What surrogate models are available in BayBE?,Can I do transfer learning in BayBE?
109,"# Surrogates
## Multi-Output Modeling
### Surrogate Replication

The simplest way to construct a multi-output surrogate is to replicate a given
single-output model architecture for each of the existing output dimensions.

To replicate a given surrogate, you can either call its
`replicate()` method or use the
[`CompositeSurrogate.from_replication()`]()
convenience constructor:

```python
from baybe.surrogates import CompositeSurrogate, GaussianProcessSurrogate

composite_a = GaussianProcessSurrogate().replicate()
composite_b = CompositeSurrogate.from_replication(GaussianProcessSurrogate())

assert composite_a == composite_b
```

However, there are very few cases where such an explicit conversion is required. Because
using a single-output surrogate model in a multi-output context would trivially fail, and
because BayBE cares deeply about its usersâ lives, it automatically performs this conversion
for you behind the scenes:

<a id=""auto-replication""></a>:class: important

When using a single-output surrogate model in a multi-output context, BayBE
automatically replicates the surrogate on the fly.

The consequence of the above is that you can use the same model object regardless
of the modeling context and its multi-output capabilities.

There is *one* notable exception where an explicit replication may still make
sense: if you want to bypass the existing multi-output mechanics of a surrogate that is
inherently multi-output compatible.",How does BayBE handle single-output surrogate models used in a multi-output context?,Can I overwrite the auto-replication method to implement my own surrogate?
111,"# Surrogates
## Extracting the Model for Advanced Study

In principle, the surrogate model does not need to be a persistent object during
Bayesian optimization since each iteration performs a new fit anyway. However, for
advanced study, such as investigating the posterior predictions, acquisition functions
or feature importance, it can be useful to directly extract the current surrogate model.

For this, BayBE provides the `get_surrogate` method, which is available for the
[`Campaign`]() or for
[recommenders]().
Below an example of how to utilize this in conjunction with the popular SHAP package:

```python
# Assuming we already have a campaign created and measurements added
data = campaign.measurements[[p.name for p in campaign.parameters]]
model = lambda x: campaign.get_surrogate().posterior(x).mean

# Apply SHAP
explainer = shap.Explainer(model, data)
shap_values = explainer(data)
shap.plots.bar(shap_values)
``` :class: note
Currently, ``get_surrogate`` always returns the surrogate model with respect to the
transformed target(s) / objective. This means that if you are using a
``SingleTargetObjective`` with a transformed target or a ``DesirabilityObjective``, the
model's output will correspond to the transformed quantities and not the original
untransformed target(s). If you are using the model for subsequent analysis this should
be kept in mind.",Can I reference the current surrogate model to use it outside of the standard optimization loop?,What method does BayBE provide to extract the current surrogate model from a campaign?
112,"# Surrogates
## Using Custom Models

BayBE goes one step further by allowing you to incorporate custom models based on the
ONNX architecture. Note however that these cannot be retrained. For a detailed
explanation on using custom models, refer to the comprehensive examples provided in the
corresponding [example folder]().",Can custom ONNX models be retrained in BayBE?,Is there support for surrogates? Could you tell me about custom model support?
113,"# Targets
Targets play a crucial role as the connection between observables measured in an
experiment and the machine learning core behind BayBE.
In general, it is expected that you create one [`Target`]()
object for each of your observables.
The way BayBE treats multiple targets is then controlled via the
[`Objective`](objectives.md).

## NumericalTarget

Besides the `name`, a [`NumericalTarget`]()
has the following attributes:

* **The optimization** `mode`: Specifies whether we want to minimize/maximize
  the target or whether we want to match a specific value.
* **Bounds**: Defines `bounds` that constrain the range of target values.
* **A** `transformation` **function**: When bounds are provided, this is
  used to map target values into the [0, 1] interval.

","What attribute of a NumericalTarget in BayBE specifies whether to minimize, maximize, or match a specific value?","I have a numerical target that ranges from inf to +inf, do I need to scale it?"
114,"# Targets
## NumericalTarget
### MIN and MAX mode

Here are two examples for simple maximization and minimization targets:

```python
from baybe.targets import NumericalTarget, TargetMode, TargetTransformation

max_target = NumericalTarget(
    name=""Target_1"",
    mode=TargetMode.MAX,  # can also be provided as string ""MAX""
)

min_target = NumericalTarget(
    name=""Target_2"",
    mode=""MIN"",  # can also be provided as TargetMode.MIN
    bounds=(0, 100),  # optional
    transformation=TargetTransformation.LINEAR,  # optional, will be applied if bounds are not None
)
```# Targets
## Limitations

#### IMPORTANT
`NumericalTarget` enables many use cases due to the real-valued nature of most
measurements. But it can also be used to model categorical targets if they are ordinal.
For example: If your experimental outcome is a categorical ranking into âbadâ,
âmediocreâ and âgoodâ, you could use a `NumericalTarget` with bounds (1, 3), where the
categories correspond to values 1, 2 and 3 respectively.
If your target category is not ordinal, the transformation into a numerical target is
not straightforward, which is a current limitation of BayBE.
We are looking into adding more target options in the future.",Can NumericalTarget in BayBE be used for modeling categorical targets?,How to specify temperature as a parameter for my experiment?
115,"# Targets
## NumericalTarget
### MATCH mode

If you want to match a desired value, the `TargetMode.MATCH` mode is the right choice.
In this mode, `bounds` are required and different transformations compared to `MIN`
and `MAX` modes are allowed.

Assume we want to instruct BayBE to match a value of 50 in a target.
We simply need to choose the bounds so that the midpoint is the desired value.
The spread of the bounds interval defines how fast the acceptability of a measurement
falls off away from the match value, also depending on the choice of `transformation`.

In the example below, `match_targetA` will treat all values < 45 and > 55 as
equally bad, while `match_targetB` is more forgiving in that it chooses a bell curve
transformation instead of a triangular one, and also uses a wider interval of bounds.
Both targets are configured such that the midpoint of `bounds` (in this case 50)
becomes the optimal value:

```python
from baybe.targets import NumericalTarget, TargetMode, TargetTransformation

match_targetA = NumericalTarget(
    name=""Target_3A"",
    mode=TargetMode.MATCH,
    bounds=(45, 55),  # mandatory in MATCH mode
    transformation=TargetTransformation.TRIANGULAR,  # optional, applied if bounds are not None
)
match_targetB = NumericalTarget(
    name=""Target_3B"",
    mode=""MATCH"",
    bounds=(0, 100),  # mandatory in MATCH mode
    transformation=""BELL"",  # can also be provided as TargetTransformation.BELL
)
```

Targets are used in nearly all [examples]().","In BayBE's NumericalTarget MATCH mode, what determines the optimal target value?",What is the keyword that I need to use when using a specific target mode?
116,"# Transfer Learning

BayBE offers the possibility to mix data from multiple, *similar but not identical*
campaigns in order to accelerate optimization â a procedure called **transfer learning**.
This feature is automatically enabled when using a
[Gaussian process surrogate model]()
in combination with a [`TaskParameter`](). :class: note
In the scientific community, the term **transfer learning** is used in many
different ways.
Within BayBE, it refers to combining data from multiple campaigns that are from similar
contexts, which we also refer as **tasks**.
Depending on the field, this might also be known as **contextual learning**.",Does BayBE support transfer learning? Does BayBE support contextual learning? Can I mix data from multiple sources in BayBE?,What does transfer learning mean in BayBE?
117,"
## Unlocking Data Treasures Through Transfer Learning

A straightforward approach to combining data from different campaigns is to quantify
the differences between their contexts via one or few explicitly measured parameters
and then constraining these parameters in the active campaign to
the relevant context.

Examples where this is possible:

* **Optimization of a chemical reaction at different temperatures:**<br />
  \\\\
  Data obtained from a chemical reaction optimized at a certain temperature can be used
  in a new campaign, where the same reaction needs to be optimized again at a different
  temperature.
* **Optimization of a simulation involving a particle size:**<br />
  \\\\
  Data obtained at a smaller particle size can be utilized when starting a new
  optimization for a larger particle size or vice versa.

In these examples, the temperature and the particle size take the
role of *aligning* the individual measurement campaigns along their corresponding
context dimension. That is, the context is static *within* each campaign
(i.e., each campaign is executed at its fixed context parameter value) but the
parameter establishes an explicit relationship between the data gathered *across*
campaigns. Transfer of knowledge from one campaign to another can thus simply happen
through the existing mechanisms of a surrogate model by feeding the context
parameter as an additional regular input to the model.

Unfortunately, there are many situations where it can be difficult to quantify the
differences between the campaigns via explicit context parameters in the first place.
This might be the case if the parameters distinguishing the contexts

1. have not been recorded and cannot be measured anymore,
2. are too many and explicitly modelling them is out of question or
3. are simply unknown.

Examples for situations where explicit quantification of the context can be difficult:

* **Cell culture media optimization for different cell types:**<br />
  \\\\
  Cell types differ among many possible descriptors, and it is not known a priori
  which ones are relevant to a newly started campaign.
* **Optimization in industrial black-box contexts:**<br />
  \\\\
  When materials (such as cell lines or complex substances) stem from customers,
  they can come uncharacterized.
* **Transfer of a complicated process to another location:**<br />
  \\\\
  The transferred machinery will likely require a new calibration/optimization, which
  could benefit from the other locationâs data. However, is not necessarily clear what
  parameters differentiate the location context.

**Transfer learning** in BayBE offers a solution for situations such as the latter,
because it abstracts each context change between campaigns into a single dimension
encoded by a [`TaskParameter`]().
Over the course of an ongoing campaign, the relationship between current campaign data
and data from previous campaigns can then be *learned* instead of requiring hard-coded
context parameters, effectively enabling you to utilize your previous data through
an additional machine learning model component.
In many situations, this can unlock data treasures coming from similar but not identical
campaigns accumulated over many years.:class: important
Because of the need to *learn* the relationship between tasks, transfer learning is
not a magic method for zero-shot learning.
For effective information transfer, it will always need data from the ongoing campaign
to understand how other campaigns' data are related.
Otherwise, it can only build upon general patterns/trends identified in the previous
campaigns, without knowing if these patterns actually reoccur in the new campaign.
(**Note:** This can still help to jump-start the new campaign since the most influential
parameter configurations from old campaigns will then drive the initial exploration.)
Overall, if correlated task data are provided, the optimization of new campaigns
can experience a dramatic speedup.:class: warning
Because of the ability to learn the task relationships, it might be tempting to add
arbitrary data to a transfer learning enabled campaigns. We caution against this, as
uncorrelated data can actually decrease the performance of the optimization. Even a
simple preliminary correlation filter to find suitable contexts can already increase
robustness.",What does BayBE use to abstract context changes between campaigns in transfer learning when explicit context parameters are unavailable?,"Can I use only old transfer-learning data in my campaign, without new experiment results?"
118,"# Transfer Learning
## The Role of the TaskParameter

The [`TaskParameter`]() is used to âmarkâ the context of individual experiments and thus
to âalignâ different campaigns along their context dimension.
The set of all possible contexts is provided upon the initialization of a
[`TaskParameter`]() by providing them as `values`.
In the following example, the context might be one of several reactors in which
a chemical experiments can be conducted.

```python
from baybe.parameters import TaskParameter

TaskParameter(name=""Reactor"", values=[""ReactorA"", ""ReactorB"", ""ReactorC""])
```

If not specified further, a campaign using the [`TaskParameter`]() as specified above
would now make recommendations for all possible values of the parameter. Using the
`active_values` argument upon initialization, this behavior can be changed such that
the `campaign` only makes recommendations for the corresponding values.

The following example models a situation where experimentation data from three
different reactors are available, but new experiments should only be conducted in
`ReactorC`.

```python
from baybe.parameters import TaskParameter

TaskParameter(
    name=""Reactor"",
    values=[""ReactorA"", ""ReactorB"", ""ReactorC""],
    active_values=[""ReactorC""],
)
```

The same pattern can be easily applied to other scenarios such as changing substrates
(while screening the same reaction conditions) or formulating mixtures for different cell lines:

```python
TaskParameter(
    name=""Substrate"",
    values=[""3,5-dimethylisoxazole"", ""benzo[d]isoxazole"", ""5-methylisoxazole""],
    active_values=[""3,5-dimethylisoxazole""],
)
TaskParameter(
    name=""Cell_Line"",
    values=[""Liver cell"", ""Heart cell"", ""Hamster brain cell""],
    active_values=[""Liver cell""],
)
```",How can you restrict BayBE experiment recommendations to only a subset of TaskParameter values?,Can I do transfer learning in BayBE?
119,"# Utilities
BayBE comes with a set of useful functions that can make your life easier in certain
scenarios.

## Search Space Memory Estimation

In search spaces that have discrete parts, the memory needed to store the respective
data can become excessively large as the number of points grows with the amount of
possible combinations arising form all discrete parameter values.

The [`SearchSpace.estimate_product_space_size`]()
and [`SubspaceDiscrete.estimate_product_space_size`]()
utilities allow estimating the memory needed to represent the discrete subspace.
They return a [`MemorySize`]() object that
contains some relevant estimates:

```python
import numpy as np

from baybe.parameters import NumericalDiscreteParameter
from baybe.searchspace import SearchSpace

# This creates 10 parameters with 20 values each.
# The resulting space would have 20^10 entries, requiring around 745 TB of memory for
# both experimental and computational representation of the search space.
parameters = [
    NumericalDiscreteParameter(name=f""p{k + 1}"", values=np.linspace(0, 100, 20))
    for k in range(10)
]

# Estimate the required memory for such a space
mem_estimate = SearchSpace.estimate_product_space_size(parameters)

# Print quantities of interest
print(""Experimental Representation"")
print(f""Estimated size: {mem_estimate.exp_rep_human_readable}"")
print(f""Estimated size in Bytes: {mem_estimate.exp_rep_bytes}"")
print(f""Expected data frame shape: {mem_estimate.exp_rep_shape}"")

print(""Computational Representation"")
print(f""Estimated size: {mem_estimate.comp_rep_human_readable}"")
print(f""Estimated size in Bytes: {mem_estimate.comp_rep_bytes}"")
print(f""Expected data frame shape: {mem_estimate.comp_rep_shape}"")
```:class: warning
{meth}`~baybe.searchspace.core.SearchSpace.estimate_product_space_size`
currently does not include the influence of potential constraints in your search space
as it is generally very hard to incorporate the effect of arbitrary constraints without
actually building the entire space. Hence, you should always **treat the number you get
as upper bound** of required memory. This can still be useful  for instance if your
estimate already is several Exabytes, it is unlikely that most computers would be able
to handle the result even if there are constraints present.:class: warning
{meth}`~baybe.searchspace.core.SearchSpace.estimate_product_space_size`
only estimates the memory required to handle the search space. **It does not estimate
the memory required during optimization**, which can be of a similar magnitude, but
generally depends on additional factors.:class: info
Continuous parameters do not influence the size of the discrete search space part as 
they do not contribute to the combinatorial configurations like discrete parameters.
Hence, they are ignored by the utility.:class: tip
If you run into issues creating large search spaces, as for instance in mixture
use cases, you should consider resorting to more specialized ways of creation by
invoking alternative search space constructors like 
{meth}`~baybe.searchspace.discrete.SubspaceDiscrete.from_dataframe`
or 
{meth}`~baybe.searchspace.discrete.SubspaceDiscrete.from_simplex`.
Instead of creating a product space first and then filtering it down
according to constraints, they offer a more direct and thus efficient path to the 
desired result, typically requiring substantially less memory. 
For example, {meth}`~baybe.searchspace.discrete.SubspaceDiscrete.from_simplex` 
includes the mixture constraint already *during* the product creation. 
In addition, BayBE can also be installed with its optional `polars` dependency 
(`pip install baybe[polars]`) that activates efficient machinery for constraint handling.",What BayBE utility can be used to estimate the memory required to represent a discrete search space?,Can I estimate the search space memory after applying my constraints?
120,"# Utilities
## Reproducibility

In some scenarios, for instance when testing your code setup, it can be useful to fix
the random seeds for all relevant engines to generate reproducible results. BayBE offers
the [`set_random_seed`]() utility for this purpose:

```python
from baybe.utils.random import set_random_seed

# Set the global random seed for all relevant engines
set_random_seed(1337)

# Assuming we have a prepared campaign
campaign.recommend(5)
```

Setting the global random seed can be undesirable if there are other packages in your
setup that might unintentionally be influenced by this. For this, BayBE offers
[`temporary_seed`]():

```python
from baybe.utils.random import temporary_seed

# Set the random seed for all relevant engines temporarily within the context
with temporary_seed(1337):
    campaign.recommend(5)
```","Does the temporary seed also affect tools like numpy, which require their own seed usually?",How can I set a temporary random seed in BayBE to ensure reproducible results without affecting other packages?
121,"# Utilities
## Adding Fake Target Measurements and Parameter Noise

When creating test scripts, it is often useful to try the recommendation loop for a few
iterations. However, this requires some arbitrary target measurements to be set. Instead
of coming up with a custom logic every time, you can use the
[`add_fake_measurements`]() utility to add fake target
measurements and the [`add_parameter_noise`]()
utility to add artificial parameter noise:

```python
from baybe.utils.dataframe import add_fake_measurements, add_parameter_noise

# Get recommendations
recommendations = campaign.recommend(5)

# Add fake target measurements and artificial parameter noise to the recommendations.
# The utilities modify the dataframes inplace.
measurements = recommendations.copy()
add_fake_measurements(measurements, campaign.targets)
add_parameter_noise(measurements, campaign.parameters)

# Now continue the loop, e.g. by adding the measurements...
```",Which utility function in BayBE can be used to add fake target measurements to a dataframe when testing recommendation loops?,How can I efficiently test my code during development without having actual target values?
122,"# BayBE — A Bayesian Back End for Design of Experiments
## 📡 Telemetry

BayBE collects anonymous usage statistics **only** for employees of Merck KGaA,
Darmstadt, Germany and/or its affiliates. The recording of metrics is turned off for
all other users and is impossible due to a VPN block. In any case, the usage statistics
do **not** involve logging of recorded measurements, targets/parameters or their names
or any project information that would allow for reconstruction of details. The user and
host machine names are anonymized with via truncated hashing.

- You can verify the above statements by studying the open-source code in the
  `telemetry` module.
- You can always deactivate all telemetry by setting the environment variable
  `BAYBE_TELEMETRY_ENABLED` to `false` or `off`. For details please consult
  [this page](https://emdgroup.github.io/baybe/stable/userguide/envvars.html#telemetry).
- If you want to be absolutely sure, you can uninstall internet related packages such
  as `opentelemetry*` or its secondary dependencies from the environment. Due to the
  inability of specifying opt-out dependencies, these are installed by default, but the
  package works without them.",How can I disable all telemetry in BayBE?,For whom does BayBE collect usage statistics?
123,"# BayBE — A Bayesian Back End for Design of Experiments
## 📡 Telemetry

BayBE collects anonymous usage statistics **only** for employees of Merck KGaA,
Darmstadt, Germany and/or its affiliates. The recording of metrics is turned off for
all other users and is impossible due to a VPN block. In any case, the usage statistics
do **not** involve logging of recorded measurements, targets/parameters or their names
or any project information that would allow for reconstruction of details. The user and
host machine names are anonymized with via truncated hashing.

- You can verify the above statements by studying the open-source code in the
  `telemetry` module.
- You can always deactivate all telemetry by setting the environment variable
  `BAYBE_TELEMETRY_ENABLED` to `false` or `off`. For details please consult
  [this page](https://emdgroup.github.io/baybe/stable/userguide/envvars.html#telemetry).
- If you want to be absolutely sure, you can uninstall internet related packages such
  as `opentelemetry*` or its secondary dependencies from the environment. Due to the
  inability of specifying opt-out dependencies, these are installed by default, but the
  package works without them.",Is there any way to disable the collection of usage statistics?,How can I disable all telemetry in BayBE?
124,"# BayBE — A Bayesian Back End for Design of Experiments
## ⚡ Quick Start
### The Optimization Loop

We can now construct a campaign object that brings all pieces of the puzzle together:

```python
from baybe import Campaign

campaign = Campaign(searchspace, objective, recommender)
```

With this object at hand, we can start our experimentation cycle.
In particular:

* We can ask BayBE to `recommend` new experiments.
* We can `add_measurements` for certain experimental settings to the campaign’s
  database.

Note that these two steps can be performed in any order.
In particular, available measurements can be submitted at any time and also several
times before querying the next recommendations.

```python
df = campaign.recommend(batch_size=3)
print(df)
```

```none
   Granularity  Pressure[bar]    Solvent
15      medium            1.0  Solvent D
10      coarse           10.0  Solvent C
29        fine            5.0  Solvent B
```

Note that the specific recommendations will depend on both the data
already fed to the campaign and the random number generator seed that is used.

After having conducted the corresponding experiments, we can add our measured
targets to the table and feed it back to the campaign:

```python
df[""Yield""] = [79.8, 54.1, 59.4]
campaign.add_measurements(df)
```

With the newly arrived data, BayBE can produce a refined design for the next iteration.
This loop would typically continue until a desired target value has been achieved in
the experiment.",How can I add measurements to the campaigns database?,How do you add new experimental measurements to a BayBE campaign?
125,"# Serialization

BayBE is shipped with a sophisticated serialization engine that allows to unstructure
its objects into basic types and seamlessly reassemble them afterward.
This enables a variety of advanced workflows, such as:

* Persisting objects for later use
* Transmission and processing outside the Python ecosystem
* Interaction with APIs and databases
* Writing “configuration” files

Some of these workflows are demonstrated in the sections below.

## JSON (de-)serialization

Most BayBE objects can be conveniently serialized into an equivalent JSON
representation by calling their
`to_json` method.
The obtained JSON string can then be deserialized via the
`from_json` method
of the corresponding class, which yields an “equivalent copy” of the original object.

For example:

```python
from baybe.parameters import CategoricalParameter

parameter = CategoricalParameter(name=""Setting"", values=[""low"", ""high""])
json_string = parameter.to_json()
reconstructed = CategoricalParameter.from_json(json_string)
assert parameter == reconstructed
```

This form of roundtrip serialization can be used, for instance, to persist objects
for long-term storage, but it also provides an easy way to “move” existing objects
between Python sessions by executing the deserializing step in a different context
than the serialization step.",How can BayBE objects be serialized and deserialized to and from JSON?,Can I transform BayBE objects into a different format?
